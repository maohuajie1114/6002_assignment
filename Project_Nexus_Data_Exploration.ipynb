{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNfyVNIUhql-"
      },
      "source": [
        "# Project Nexus: HR Attrition Prediction & Organizational Network Analysis\n",
        "\n",
        "## 1. Project Overview\n",
        "**Objective:**\n",
        "Traditional HR analytics only predict *who* might leave. **Project Nexus** goes a step further by identifying **\"Attrition Contagion\"**—predicting how the departure of key influencers (high PageRank employees) can trigger a chain reaction of resignations within the team.\n",
        "\n",
        "**Our Novel Approach:**\n",
        "We integrate **Machine Learning (XGBoost)** with **Organizational Network Analysis (ONA)**. By constructing a simulated social network based on employee homophily, we can quantify \"hidden\" social capital and identify high-risk clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00nWDkSUZU5p"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas numpy matplotlib seaborn networkx scikit-learn\n",
        "!pip install xgboost\n",
        "!pip install shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w44gGSWb8odC"
      },
      "source": [
        "# 2. Data Architecture Phase\n",
        "\n",
        "**Author:** Luo Yurou\n",
        "\n",
        "This notebook covers **Phase 1** of the project: *Data Engineering and Network Construction*. The pipeline is divided into four core modules:\n",
        "\n",
        "### Module 1: Data Initialization & Exploratory Analysis\n",
        "* **Preprocessing:** Dimensionality reduction by removing zero-variance features (`Over18`, `EmployeeCount`, `StandardHours`) and ensuring data integrity via forward-fill imputation.\n",
        "* **EDA:** Analyzing class imbalance to justify synthetic data generation (SMOTE) and utilizing heatmaps to identify multicollinearity for cleaner feature selection.\n",
        "\n",
        "### Module 2: Network Topology Construction (The Innovation)\n",
        "> *\"Simulating the Invisible\": Constructing a synthetic Homophily Graph to model information flow where no raw network data existed.*\n",
        "\n",
        "* **Logic:** Edges are established using a **Hard Constraint** (Departmental matching) and a **Soft Constraint** (Attribute Similarity).\n",
        "* **Methodology:** Utilizing K-Nearest Neighbors ($k=5$) on scaled `Age`, `JobLevel`, and `Tenure` to link employees with similar professional profiles.\n",
        "\n",
        "### Module 3: Advanced Feature Engineering\n",
        "* **Individual \"Golden Features\":** Deriving behavioral metrics including `Tenure_Ratio` (Loyalty), `Income_Per_Age` (Fairness), `Burnout_Index` (Stress), and `Relative_Sat_to_Dept` (Isolation/Relative Deprivation).\n",
        "* **Network Metrics:** Calculating `Neighbor_Risk_Score` to model \"Attrition Contagion\", `PageRank_Score` to identify influential nodes, and `Clustering_Coef` to measure social community tightness.\n",
        "\n",
        "### Module 4: Visualization & Validation\n",
        "* **Visual Validation:** Using Boxplots to verify distribution differences in derived features between attrition groups.\n",
        "* **Network Visualization:** Rendering the topology using a Force-Directed Graph (Spring Layout), where **Node Color** indicates Department and **Node Size** represents Influence (PageRank)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T9f9faUct_L"
      },
      "source": [
        "## Section 1: Data Preprocessing & Cleaning\n",
        "\n",
        "**Objective**\n",
        "Transform the raw IBM HR Analytics dataset into a clean, robust format suitable for both Network Analysis and Machine Learning pipelines.\n",
        "\n",
        "**Key Actions**\n",
        "\n",
        "1.  **Dimensionality Reduction (Noise Removal)**\n",
        "    * **Action:** Removed columns `Over18`, `EmployeeCount`, and `StandardHours`.\n",
        "    * > **Reasoning:** These features exhibit **zero variance** (i.e., every employee has the exact same value). They offer no distinctive information for the predictive model and serve only to add computational noise.\n",
        "\n",
        "2.  **Data Integrity Check**\n",
        "    * **Action:** Implemented a robust check to detect and fill missing values (utilizing forward fill).\n",
        "    * **Goal:** Ensure the dataset is \"Network-Ready\" to prevent errors during edge construction in the graph phase.\n",
        "\n",
        "3.  **Reproducibility Protocol**\n",
        "    * **Action:** Set `np.random.seed(42)`.\n",
        "    * **Goal:** Guarantees that all stochastic processes—such as Network Generation and SMOTE sampling—produce consistent, reproducible results for every run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHVd9oi-bVlM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Set random seed for reproducibility (Professional Practice)\n",
        "np.random.seed(42)\n",
        "\n",
        "def load_and_clean_data(filepath):\n",
        "    \"\"\"\n",
        "    Load data and perform basic cleaning.\n",
        "    Remove columns with zero variance (useless for prediction).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(filepath)\n",
        "    print(f\"Original Data Shape: {df.shape}\")\n",
        "\n",
        "    # Useless columns specific to the IBM dataset (all values are 'Yes' or standard hours)\n",
        "    useless_cols = ['Over18', 'EmployeeCount', 'StandardHours']\n",
        "    df_clean = df.drop(columns=useless_cols, errors='ignore')\n",
        "\n",
        "    # Check for missing values\n",
        "    if df_clean.isnull().sum().sum() > 0:\n",
        "        print(\"Warning: Missing values found, filling...\")\n",
        "        df_clean.fillna(method='ffill', inplace=True) # Simple fill, modify if necessary\n",
        "\n",
        "    print(f\"Cleaned Data Shape: {df_clean.shape}\")\n",
        "    return df_clean\n",
        "\n",
        "# Execution\n",
        "df = load_and_clean_data('/content/WA_Fn-UseC_-HR-Employee-Attrition.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEIYj6tS4hk8"
      },
      "source": [
        "## Section 2: Exploratory Data Analysis (EDA)\n",
        "\n",
        "**Objective**\n",
        "Visualize data distributions and variable relationships to inform strategic model selection.\n",
        "\n",
        "**Key Actions**\n",
        "\n",
        "* **Class Imbalance Analysis**\n",
        "    * Visualized the target variable (`Attrition`) to demonstrate the skew between employees who stay vs. those who leave.\n",
        "    * **Insight:** Confirms the necessity for synthetic data generation techniques (like SMOTE) to prevent model bias toward the majority class.\n",
        "\n",
        "* **Feature Correlation Analysis**\n",
        "    * Utilized heatmaps to identify multicollinearity.\n",
        "    * **Insight:** Helps determine which features are redundant (e.g., strong correlation between `JobLevel` and `MonthlyIncome`), allowing for cleaner feature selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqafvmJ_e9Up"
      },
      "outputs": [],
      "source": [
        "def plot_eda_charts(df):\n",
        "    \"\"\"\n",
        "    Generates EDA charts to visualize data distribution and relationships.\n",
        "    Saves high-res images to the Colab sidebar for PPT inclusion.\n",
        "    \"\"\"\n",
        "    # Set visual style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # --- 1. Target Variable Distribution ---\n",
        "    # Objective: Visualize Class Imbalance.\n",
        "    # This chart proves to Member B why SMOTE is strictly necessary later.\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    ax = sns.countplot(x='Attrition', data=df, palette='viridis')\n",
        "    plt.title('Distribution of Target Variable (Attrition)', fontsize=15)\n",
        "    plt.xlabel('Attrition Status')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Add counts on top of bars for clarity\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'{p.get_height()}', (p.get_x() + 0.35, p.get_height() + 10))\n",
        "\n",
        "    # Save to local runtime disk (Sidebar)\n",
        "    plt.savefig('attrition_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved to sidebar: attrition_distribution.png\")\n",
        "\n",
        "    plt.show()\n",
        "    print(\"Insight: The severe imbalance (16% vs 84%) confirms we need synthetic data generation (SMOTE).\")\n",
        "\n",
        "    # --- 2. Feature Correlation Heatmap ---\n",
        "    # Objective: Identify Multicollinearity.\n",
        "    # Helps us decide which features might be redundant (e.g., JobLevel vs MonthlyIncome).\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Select numerical columns only\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    corr = numeric_df.corr()\n",
        "\n",
        "    # Mask the upper triangle to reduce visual clutter\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "    sns.heatmap(corr, mask=mask, cmap='coolwarm', vmax=1, center=0,\n",
        "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)\n",
        "\n",
        "    plt.title('Feature Correlation Heatmap', fontsize=15)\n",
        "\n",
        "    # Save to local runtime disk (Sidebar)\n",
        "    plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved to sidebar: correlation_heatmap.png\")\n",
        "\n",
        "    plt.show()\n",
        "    print(\"Insight: Darker red indicates strong positive correlation. This guides our feature selection strategy.\")\n",
        "\n",
        "# Execute\n",
        "plot_eda_charts(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLyO4gLpzz4D"
      },
      "source": [
        "## Section 3: Feature Engineering\n",
        "\n",
        "This section details the construction of advanced metrics designed to capture **Individual Behavioral Patterns** and **Social Network Dynamics**.\n",
        "\n",
        "### 1) Derived \"Golden Features\" (Individual Level)\n",
        "We constructed four key features based on EDA insights to enhance the model's predictive power:\n",
        "\n",
        "* **`Tenure_Ratio` (Loyalty)**: $\\text{Tenure_Ratio} = \\frac{\\text{TotalWorkingYears}}{\\text{NumCompaniesWorked}}$\n",
        "    * **Logic:** Identifies \"Job Hoppers\" vs. loyal employees. A lower ratio suggests frequent switching.\n",
        "\n",
        "* **`Income_Per_Age` (Fairness)**: $\\text{Income_Per_Age} = \\frac{\\text{MonthlyIncome}}{\\text{Age}}$\n",
        "    * **Logic:** Highlights potential underpayment relative to age cohort. Low values may trigger attrition due to perceived unfairness.\n",
        "\n",
        "* **`Burnout_Index` (Stress)**: $\\text{Burnout_Index} = \\frac{\\text{PerformanceRating}}{\\text{JobSatisfaction}}$\n",
        "    * **Logic:** Captures high performers who are dissatisfied (**High Flight Risk**). High input (rating) but low output (satisfaction) equals burnout.\n",
        "\n",
        "* **`Relative_Sat_to_Dept` (Isolation)**: $\\text{Relative_Sat} = \\text{JobSatisfaction} - \\text{DeptMean(JobSatisfaction)}$\n",
        "    * **Logic:** Measures the psychological gap compared to direct peers (**Relative Deprivation**). Being less satisfied than one's teammates is a strong predictor of leaving.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Organizational Network Analysis (Social Structural Level)\n",
        "We constructed a synthetic **Homophily Graph** (using KNN on Scaled `Age`, `JobLevel`, and `Tenure` within Departments) to model the **\"Attrition Contagion\"** effect.\n",
        "\n",
        "* **`Neighbor_Risk_Score` (Contagion):** The percentage of an employee's direct connections who have already left. This captures the \"ripple effect\" of turnover.\n",
        "* **`PageRank_Score` (Influence):** Identifies informal leaders or central nodes whose departure might destabilize the entire team structure.\n",
        "* **`Clustering_Coef` (Community):** Measures social tightness. High clustering coefficients suggest tight-knit groups that often act as \"retention anchors.\"\n",
        "\n",
        "### 3) Validation Strategy\n",
        "* **Visual Validation:** Post-generation, we utilize **Boxplots** to verify that these new features show significant distribution differences between the `Attrition=Yes` and `Attrition=No` groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHrtUXZ0baUZ"
      },
      "outputs": [],
      "source": [
        "def create_derived_features(df):\n",
        "    \"\"\"\n",
        "    Construct derived features based on EDA validation.\n",
        "    Target: Enhance model predictive power by capturing proven behavioral patterns.\n",
        "    \"\"\"\n",
        "    df_eng = df.copy()\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. Tenure Ratio (Loyalty/Stability) - [Validated: Effective]\n",
        "    # Formula: TotalWorkingYears / (NumCompaniesWorked + 1)\n",
        "    # Logic: Calculates average tenure per employer.\n",
        "    # Low values indicate a \"Job Hopper\" risk.\n",
        "    # ---------------------------------------------------------\n",
        "    df_eng['Tenure_Ratio'] = df_eng['TotalWorkingYears'] / (df_eng['NumCompaniesWorked'] + 1)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. Income Stability (Age-Adjusted Income) - [Validated: Strong Feature]\n",
        "    # Formula: MonthlyIncome / Age\n",
        "    # Logic: Normalizes income relative to age.\n",
        "    # Identifies if an employee is underpaid compared to their age cohort.\n",
        "    # ---------------------------------------------------------\n",
        "    df_eng['Income_Per_Age'] = df_eng['MonthlyIncome'] / df_eng['Age']\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. Burnout Index (High Performance / Low Satisfaction) - [New: Strongest Feature]\n",
        "    # Formula: PerformanceRating / JobSatisfaction\n",
        "    # Logic: Captures \"Star Performers\" who are unhappy.\n",
        "    # High value = High Performance but Low Satisfaction (High Attrition Risk).\n",
        "    # ---------------------------------------------------------\n",
        "    if 'PerformanceRating' in df_eng.columns and 'JobSatisfaction' in df_eng.columns:\n",
        "        df_eng['Burnout_Index'] = df_eng['PerformanceRating'] / df_eng['JobSatisfaction']\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 4. Relative Satisfaction (Peer Comparison) - [New: Strong Feature]\n",
        "    # Formula: Individual JobSatisfaction - Department Average\n",
        "    # Logic: Measures if an employee is happier or sadder than their direct peers.\n",
        "    # Negative value = \"I hate it here, but everyone else seems fine\" (Isolation).\n",
        "    # ---------------------------------------------------------\n",
        "    if 'Department' in df_eng.columns and 'JobSatisfaction' in df_eng.columns:\n",
        "        # Calculate mean satisfaction per department\n",
        "        dept_means = df_eng.groupby('Department')['JobSatisfaction'].transform('mean')\n",
        "        df_eng['Relative_Sat_to_Dept'] = df_eng['JobSatisfaction'] - dept_means\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Cleanup: Remove the ineffective feature if it exists\n",
        "    # ---------------------------------------------------------\n",
        "    if 'Satisfaction_Gap' in df_eng.columns:\n",
        "        df_eng.drop('Satisfaction_Gap', axis=1, inplace=True)\n",
        "        print(\"Note: Removed 'Satisfaction_Gap' due to low predictive power.\")\n",
        "\n",
        "    print(f\"Feature Engineering Complete.\")\n",
        "    print(f\"Added/Retained: ['Tenure_Ratio', 'Income_Per_Age', 'Burnout_Index', 'Relative_Sat_to_Dept']\")\n",
        "\n",
        "    return df_eng\n",
        "\n",
        "# Execution\n",
        "df = create_derived_features(df)\n",
        "\n",
        "# Check results\n",
        "print(df[['Tenure_Ratio', 'Income_Per_Age', 'Burnout_Index', 'Relative_Sat_to_Dept']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp2QIMqNpiF1"
      },
      "outputs": [],
      "source": [
        "def plot_final_features_analysis(df):\n",
        "    \"\"\"\n",
        "    Visualizes and analyzes the 4 identified \"Golden Features\" and saves the plot.\n",
        "    \"\"\"\n",
        "    # Set plotting style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Define the list of final features to analyze\n",
        "    target_features = [\n",
        "        'Burnout_Index',        # Burnout Index (Strongest predictor)\n",
        "        'Income_Per_Age',       # Income Per Age (Very strong)\n",
        "        'Relative_Sat_to_Dept', # Relative Satisfaction to Dept (Strong)\n",
        "        'Tenure_Ratio'          # Tenure Ratio/Loyalty (Medium-Strong)\n",
        "    ]\n",
        "\n",
        "    # Filter out columns not present in the dataset (to prevent errors)\n",
        "    valid_features = [col for col in target_features if col in df.columns]\n",
        "\n",
        "    if not valid_features:\n",
        "        print(\" New features not found. Please run the 'create_derived_features' function first!\")\n",
        "        return\n",
        "\n",
        "    # Create figure (1 row, N columns)\n",
        "    fig, axes = plt.subplots(1, len(valid_features), figsize=(5 * len(valid_features), 6))\n",
        "\n",
        "    # If there is only one feature, 'axes' is not a list; convert it for iteration\n",
        "    if len(valid_features) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    # Loop to plot boxplots\n",
        "    for i, col in enumerate(valid_features):\n",
        "        # Draw boxplot: visualize distribution, median, and outliers\n",
        "        sns.boxplot(\n",
        "            x='Attrition',\n",
        "            y=col,\n",
        "            data=df,\n",
        "            ax=axes[i],\n",
        "            palette='Set2',\n",
        "            showfliers=True # Show outliers, this is important\n",
        "        )\n",
        "\n",
        "        # Set titles and labels\n",
        "        axes[i].set_title(f'{col}\\n(Impact on Attrition)', fontsize=14, fontweight='bold')\n",
        "        axes[i].set_xlabel('Attrition (Yes=Left, No=Stay)')\n",
        "        axes[i].set_ylabel('Feature Value')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure\n",
        "    filename = 'final_features_validation.png'\n",
        "    plt.savefig(filename, dpi=300)\n",
        "    print(f\" Chart saved to: {filename}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# --- Run Visualization ---\n",
        "plot_final_features_analysis(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9lk2aUccy67"
      },
      "source": [
        "## Section 4: Organizational Network Construction (Novelty)\n",
        "\n",
        "This section details the engineering of a synthetic social graph to model **\"Attrition Contagion\"** and **Influence Dynamics** where no raw network data existed.\n",
        "\n",
        "### 1) Network Topology Construction (Methodology)\n",
        "We imposed specific constraints to simulate realistic information flow within the organization:\n",
        "\n",
        "* **Hard Constraint (Departmental)**: $\\text{Edge}(u, v) \\iff \\text{Dept}_u = \\text{Dept}_v$\n",
        "    * **Logic:** Employees primarily interact with peers in their own department. Edges are strictly restricted to intra-departmental connections.\n",
        "\n",
        "* **Soft Constraint (Attribute Similarity)**: $\\text{Weight}_{uv} = \\text{KNN}(\\text{Age, JobLevel, Tenure})$\n",
        "    * **Logic:** Within departments, we use **K-Nearest Neighbors ($k=5$)** to link employees who share similar professional profiles.\n",
        "\n",
        "* **Critical Feature Scaling**: $X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma}$ (StandardScaler)\n",
        "    * **Logic:** Before KNN, we normalized features. Without scaling, large-magnitude variables (e.g., `Age` $\\approx 40$) would dominate small-range variables (e.g., `JobLevel` $\\approx 2$), distorting distance calculations.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Derived Network Metrics (Social Structural Level)\n",
        "We calculated structural metrics to capture social dynamics and the contagion effect:\n",
        "\n",
        "* **`Neighbor_Risk_Score` (Contagion)**: $\\text{Risk}_i = \\frac{\\sum_{j \\in \\text{Neighbors}} \\text{Attrition}_j}{\\text{Total Neighbors}_i}$\n",
        "    * **Logic:** Measures the \"ripple effect\" of turnover. The percentage of an employee's direct connections who have already left. If your closest peers leave, you are statistically more likely to leave.\n",
        "\n",
        "* **`PageRank_Score` (Influence)**: $\\text{PR}_i = \\alpha \\sum \\frac{\\text{PR}_j}{L_j} + \\frac{1-\\alpha}{N}$\n",
        "    * **Logic:** Identifies \"Informal Leaders\" or central nodes. Their departure might destabilize the team structure. Visualized as **Node Size** in the graph.\n",
        "\n",
        "* **`Clustering_Coef` (Community)**: $\\text{Clustering}_i = \\frac{2 \\times \\text{Triangles}_i}{\\text{Degree}_i (\\text{Degree}_i - 1)}$\n",
        "    * **Logic:** Measures social tightness. High clustering coefficients suggest tight-knit cliques that often act as \"retention anchors,\" making it harder for individuals to drift away.\n",
        "\n",
        "### 3) Visualization Strategy\n",
        "* **Force-Directed Graph**: We utilize a spring layout to visually validate natural clusters.\n",
        "* **Visual Encoding**: **Node Color** represents Department; **Node Size** represents Influence (PageRank)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j0Xnv7AbfwC"
      },
      "outputs": [],
      "source": [
        "def build_organizational_network_final(df, n_neighbors=5):\n",
        "    \"\"\"\n",
        "    Constructs the organizational network based on Homophily with Feature Scaling.\n",
        "\n",
        "    Logic: Employees in the same department with similar Age, JobLevel, and Tenure\n",
        "    are linked. Scaling is applied to ensure all features contribute equally.\n",
        "\n",
        "    Ref: Project Outline Section 2 [Source: 18]\n",
        "    \"\"\"\n",
        "    print(\"Building Organizational Network Graph (Final Version)...\")\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(df['EmployeeNumber']) # Nodes are Employee IDs\n",
        "\n",
        "    # 1. Feature Scaling (CRITICAL FIX)\n",
        "    # Without scaling, Age (e.g., 40) dominates JobLevel (e.g., 2).\n",
        "    # We use StandardScaler to put them on the same scale (mean=0, std=1).\n",
        "    feature_cols = ['Age', 'TotalWorkingYears', 'JobLevel', 'YearsAtCompany']\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Create a temporary scaled dataframe to calculate distances\n",
        "    # We do NOT overwrite the original df, as we need raw values later.\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "    # 2. Build Connections by Department\n",
        "    departments = df['Department'].unique()\n",
        "\n",
        "    for dept in departments:\n",
        "        # Filter employees in the current department\n",
        "        # We need both the scaled data (for KNN) and original IDs (for linking)\n",
        "        dept_mask = df['Department'] == dept\n",
        "        dept_data_scaled = df_scaled.loc[dept_mask, feature_cols]\n",
        "        dept_employee_ids = df.loc[dept_mask, 'EmployeeNumber'].values\n",
        "\n",
        "        # Skip if department is too small for KNN\n",
        "        if len(dept_data_scaled) < n_neighbors:\n",
        "            continue\n",
        "\n",
        "        # 3. Run KNN\n",
        "        knn = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')\n",
        "        knn.fit(dept_data_scaled)\n",
        "        distances, indices = knn.kneighbors(dept_data_scaled)\n",
        "\n",
        "        # 4. Add Edges\n",
        "        for i in range(len(dept_data_scaled)):\n",
        "            source_id = dept_employee_ids[i]\n",
        "\n",
        "            # 'indices[i]' returns the index relative to the dept_data_scaled array\n",
        "            for neighbor_idx in indices[i]:\n",
        "                # Exclude self-loop\n",
        "                if i != neighbor_idx:\n",
        "                    target_id = dept_employee_ids[neighbor_idx]\n",
        "                    G.add_edge(source_id, target_id)\n",
        "\n",
        "    print(f\" Network Built.\")\n",
        "    print(f\"   - Nodes: {G.number_of_nodes()}\")\n",
        "    print(f\"   - Edges: {G.number_of_edges()}\")\n",
        "\n",
        "    return G\n",
        "\n",
        "def calculate_network_metrics_final(df, G):\n",
        "    \"\"\"\n",
        "    Calculates network metrics including Influence, Contagion Risk, and Community Tightness.\n",
        "    Merges these features back into the main DataFrame for the AI model.\n",
        "\n",
        "    Ref: Project Outline Section 2 & 4 [Source: 80, 82, 85]\n",
        "    \"\"\"\n",
        "    print(\"Calculating Network Metrics...\")\n",
        "\n",
        "    # --- A. Structural Metrics ---\n",
        "\n",
        "    # 1. PageRank (Influence) [Source: 82]\n",
        "    # Who are the \"key players\" or informal leaders?\n",
        "    try:\n",
        "        pagerank = nx.pagerank(G, alpha=0.85)\n",
        "    except:\n",
        "        pagerank = {n: 0 for n in G.nodes()}\n",
        "\n",
        "    # 2. Clustering Coefficient (Community Tightness)\n",
        "    # High value = \"My friends are also friends with each other\".\n",
        "    # Harder to leave a tight clique.\n",
        "    clustering = nx.clustering(G)\n",
        "\n",
        "    # 3. Betweenness Centrality (Bridge Index) [Source: 83]\n",
        "    # High value = Connects different groups.\n",
        "    betweenness = nx.betweenness_centrality(G, k=100, seed=42)\n",
        "\n",
        "    # 4. Degree Centrality (Popularity) [Source: 84]\n",
        "    degree = nx.degree_centrality(G)\n",
        "\n",
        "    # --- B. Contagion Metrics (The Novelty) ---\n",
        "\n",
        "    # 5. Neighbor Risk Score (Attrition Contagion) [Source: 85]\n",
        "    # Measures the percentage of direct friends who have already left.\n",
        "    # Essential for the \"Attrition Contagion\" analysis.\n",
        "\n",
        "    # Ensure Attrition is mapped to 0/1 for calculation\n",
        "    # Handle cases where Attrition might still be 'Yes'/'No' or already numeric\n",
        "    if df['Attrition'].dtype == 'object':\n",
        "        attrition_map = df.set_index('EmployeeNumber')['Attrition'].map({'Yes': 1, 'No': 0}).to_dict()\n",
        "    else:\n",
        "        attrition_map = df.set_index('EmployeeNumber')['Attrition'].to_dict()\n",
        "\n",
        "    neighbor_risk = {}\n",
        "    for node in G.nodes():\n",
        "        neighbors = list(G.neighbors(node))\n",
        "        if len(neighbors) > 0:\n",
        "            neighbor_vals = [attrition_map.get(n, 0) for n in neighbors]\n",
        "            neighbor_risk[node] = sum(neighbor_vals) / len(neighbors)\n",
        "        else:\n",
        "            neighbor_risk[node] = 0.0\n",
        "\n",
        "    # --- C. Isolation Metric ---\n",
        "\n",
        "    # 6. Is_Isolated\n",
        "    # Employees with no connections in the graph (high risk of leaving due to loneliness)\n",
        "    is_isolated = {node: 1 if degree.get(node, 0) == 0 else 0 for node in G.nodes()}\n",
        "\n",
        "    # --- Merge Everything ---\n",
        "\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'EmployeeNumber': list(G.nodes()),\n",
        "        'PageRank_Score': [pagerank.get(n, 0) for n in G.nodes()],\n",
        "        'Betweenness_Score': [betweenness.get(n, 0) for n in G.nodes()],\n",
        "        'Clustering_Coef': [clustering.get(n, 0) for n in G.nodes()],\n",
        "        'Neighbor_Risk_Score': [neighbor_risk.get(n, 0) for n in G.nodes()],\n",
        "        'Is_Isolated': [is_isolated.get(n, 0) for n in G.nodes()]\n",
        "    })\n",
        "\n",
        "    # Merge back to main dataframe\n",
        "    df_final = pd.merge(df, metrics_df, on='EmployeeNumber', how='left')\n",
        "\n",
        "    # Fill NaNs for any employees not in the graph (safety check)\n",
        "    fill_cols = ['PageRank_Score', 'Betweenness_Score', 'Clustering_Coef', 'Neighbor_Risk_Score', 'Is_Isolated']\n",
        "    df_final[fill_cols] = df_final[fill_cols].fillna(0)\n",
        "\n",
        "    print(\" Metrics merged successfully.\")\n",
        "    return df_final, G\n",
        "\n",
        "# --- Execution ---\n",
        "# 1. Build the network with Scaled Features\n",
        "G = build_organizational_network_final(df)\n",
        "\n",
        "# 2. Calculate Metrics (including Contagion Risk)\n",
        "df_final, G = calculate_network_metrics_final(df, G)\n",
        "\n",
        "# 3. Preview to verify 'Neighbor_Risk_Score' is present (Critical for Outline)\n",
        "print(df_final[['EmployeeNumber', 'Neighbor_Risk_Score', 'Clustering_Coef', 'Attrition']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGmrqMw_bhlb"
      },
      "outputs": [],
      "source": [
        "def visualize_network_optimized(G, df_final, save_path='network_graph.png'):\n",
        "    \"\"\"\n",
        "    Plots the Force-Directed Graph with optimized performance and aesthetics.\n",
        "    \"\"\"\n",
        "    print(\"Generating Network Visualization (this may take a moment)...\")\n",
        "    plt.figure(figsize=(14, 10)) # Larger canvas\n",
        "\n",
        "    # --- 1. Optimization: Create Mappings (O(1) lookup) ---\n",
        "    pagerank_map = df_final.set_index('EmployeeNumber')['PageRank_Score'].to_dict()\n",
        "    dept_series = df_final.set_index('EmployeeNumber')['Department']\n",
        "    dept_map_raw = dept_series.to_dict()\n",
        "\n",
        "    # Create color mapping for Departments\n",
        "    unique_depts = dept_series.unique()\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_depts)))\n",
        "    dept_color_dict = dict(zip(unique_depts, colors))\n",
        "\n",
        "    # --- 2. Prepare Drawing Lists ---\n",
        "    node_sizes = [pagerank_map.get(n, 0) * 8000 for n in G.nodes()]\n",
        "    node_colors = [dept_color_dict.get(dept_map_raw.get(n), 'grey') for n in G.nodes()]\n",
        "\n",
        "    # --- 3. Layout Algorithm ---\n",
        "    pos = nx.spring_layout(G, k=0.12, iterations=30, seed=42)\n",
        "\n",
        "    # --- 4. Draw ---\n",
        "    nx.draw_networkx_nodes(\n",
        "        G, pos,\n",
        "        node_size=node_sizes,\n",
        "        node_color=node_colors,\n",
        "        alpha=0.85,\n",
        "        linewidths=0.5,\n",
        "        edgecolors='white'\n",
        "    )\n",
        "\n",
        "    nx.draw_networkx_edges(\n",
        "        G, pos,\n",
        "        alpha=0.15,\n",
        "        edge_color='gray',\n",
        "        width=0.5\n",
        "    )\n",
        "\n",
        "    # --- 5. Add Legend ---\n",
        "    legend_handles = [mpatches.Patch(color=color, label=dept) for dept, color in dept_color_dict.items()]\n",
        "    plt.legend(handles=legend_handles, title=\"Department\", loc='upper right', bbox_to_anchor=(1.15, 1))\n",
        "\n",
        "    plt.title(\"Organizational Network Topology\\n(Node Size = Influence/PageRank)\", fontsize=16, fontweight='bold')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Save & Show\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\" Network Graph saved to: {save_path}\")\n",
        "\n",
        "visualize_network_optimized(G, df_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwA8r84xFaBI"
      },
      "source": [
        "# 3. AI Model Design & Evaluation\n",
        "\n",
        "**Author:**  **LYU JIZHOU**\n",
        "\n",
        "This notebook covers **Phase 2** of the project: *AI Model Architect and Causal Inference*. The pipeline is divided into four core modules:\n",
        "\n",
        "**Objective:**\n",
        "Integrate network features (PageRank) with tabular features to build a high-accuracy employee attrition prediction model, using SMOTE and XGBoost, and provide explanations from both model performance and causal inference.\n",
        "\n",
        "### Module 1: Feature Engineering & Data Balancing\n",
        "* **Categorical Encoding:** Convert non-numeric features into a format acceptable by the model using Encoding Techniques.\n",
        "\n",
        "* **Feature Integration:** Combine network features (e.g., PageRank scores, node degrees) with tabular features to provide the model with structural information about the organization.\n",
        "\n",
        "* **Handling Imbalanced Data:** The proportion of employees who leave is approximately 16%, while those who stay account for 84%. Apply SMOTE (Synthetic Minority Oversampling Technique) to generate synthetic samples for the minority class, balancing the training set.\n",
        "\n",
        "### Module 2: Model Construction\n",
        "* **Model Choice:** Use XGBoost (Gradient Boosting Decision Tree) for its high accuracy and strong resistance to overfitting.\n",
        "\n",
        "* **Model Training:** Feed the fused features into XGBoost for model training.\n",
        "\n",
        "* **Parameter Selection & Fine-tuning:** Tune hyperparameters such as max_depth, learning_rate, and n_estimators to achieve optimal performance.\n",
        "\n",
        "### Module 3: Model Evaluation & Performance\n",
        "* **Precision / Recall / F1-score:** Focus on the accuracy of predicting the minority class.\n",
        "\n",
        "* **Learning Curve:** Plot training vs validation error over sample size to detect underfitting or overfitting.\n",
        "\n",
        "* **ROC Curve:** Evaluate classification performance, particularly the ability to detect the minority class.\n",
        "\n",
        "* **Feature Impact:** Highlight top features contributing to model predictions.\n",
        "\n",
        "### Module 4: Causal Analysis\n",
        "* **Treatment & Outcome:** Select key features and outcome variables for causal estimation.\n",
        "\n",
        "* **Causal Estimation:** Use DoWhy or CausalNex to compute Average Treatment Effect (ATE) or other causal metrics.\n",
        "\n",
        "* **Interpretation:** Analyze the causal impact of selected features independently from model prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lUVjRV8FaBJ"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install imbalanced-learn\n",
        "!pip install dowhy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKXwcy0VFaBJ"
      },
      "source": [
        "## Module 1: Feature Engineering & Data Balancing\n",
        "\n",
        "**Objective**\n",
        "Prepare input features suitable for the model and handle class imbalance.\n",
        "\n",
        "**Output**\n",
        "Cleaned, encoded, and balanced training feature matrix and labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbVqyIsNFaBJ"
      },
      "source": [
        "### Split Train / Test Before Encoding\n",
        "\n",
        "**Prevent Data Leakage**\n",
        "\n",
        "If you encode the entire dataset before splitting, information from the test set can leak into the training set. This causes overfitting and gives overly optimistic performance metrics.\n",
        "\n",
        "**Ensure Independent Encoding**\n",
        "\n",
        "Encoders should be fitted on the training set only. The test set should only use the rules derived from training to simulate real-world scenarios.\n",
        "\n",
        "**Handle Unseen Categories**\n",
        "\n",
        "Test data may contain categories not seen in training. Splitting first allows to properly handle unknown categories (e.g., unknown_value=-1 or handle_unknown='ignore')."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFQazfNwFaBJ"
      },
      "source": [
        "### Data Scaling\n",
        "**Not Necessary**\n",
        "* Tree-based models are scale-invariant\n",
        "  * Models like Decision Trees, Random Forest, and XGBoost do not rely on the magnitude of features.\n",
        "  * Splits are based on feature order and thresholds, not absolute values.\n",
        "\n",
        "* SMOTE works in feature space, not sensitive to scale for tree-based models\n",
        "  * SMOTE generates synthetic samples by interpolating between minority class points.\n",
        "  * Minor differences in scale do not impact splits for tree-based models.\n",
        "\n",
        "* Avoid unnecessary preprocessing\n",
        "  * Scaling adds computational cost and complexity.\n",
        "  * Since your pipeline uses tree-based classifiers, scaling does not bring benefits.\n",
        "\n",
        "* Feature importance ranking remains valid\n",
        "  * Tree-based models compute feature importance based on how much a feature reduces impurity (e.g., Gini, entropy).\n",
        "  * Scaling the features does not change the order of feature importance, so relative ranking remains the same.\n",
        "\n",
        "**Summary**\n",
        "* For tree-based models, scaling is unnecessary because splits depend on thresholds, SMOTE works directly, and feature importance ranking is unaffected by feature magnitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dlqla70FaBJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "def advanced_data_processing(df):\n",
        "  \"\"\"\n",
        "  Advanced Data Preprocessing Pipeline for Employee Attrition Prediction\n",
        "\n",
        "  This function implements a complete and leakage-safe data preprocessing pipeline to prepare structured data for machine learning models.\n",
        "\n",
        "  Steps:\n",
        "    1. Drop irrelevant columns\n",
        "    2. Split features and target\n",
        "    3. Split train/test sets\n",
        "    4. Encode categorical variables\n",
        "      - Ordinal Encoding\n",
        "      - One-Hot Encoding\n",
        "      - Frequency Encoding\n",
        "      - Binary Encoding\n",
        "    5. Return processed train/test sets\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # 1. Drop irrelevant columns\n",
        "\n",
        "\n",
        "  # Columns that do not help predict employee attrition and may introduce noise.\n",
        "  # EmployeeNumber is a unique identifier with no predictive value\n",
        "\n",
        "  columns_to_drop = ['EmployeeNumber']\n",
        "  df_clean = df.drop(columns=columns_to_drop)\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # 2. Split features and target\n",
        "\n",
        "\n",
        "  # Attrition is the target column (Yes/No)\n",
        "  X = df_clean.drop(columns=['Attrition'])\n",
        "  y = df_clean['Attrition']\n",
        "  print(f\"Features shape: {X.shape}\")\n",
        "  print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # 3. Split train/test sets\n",
        "\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "      X,\n",
        "      y,\n",
        "      test_size=0.2,\n",
        "      random_state=42,  # For reproducibility\n",
        "      stratify=y  # stratify=y ensures the same target distribution in train/test\n",
        "  )\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # 4. Encode variable\n",
        "\n",
        "\n",
        "  # 4.1 Label Encoding\n",
        "  # BusinessTravel has an obvious hierarchy, code the ordered features to preserve the rank information\n",
        "  if 'BusinessTravel' in X_train.columns:\n",
        "    travel_order = [[\"Non-Travel\", \"Travel_Rarely\", \"Travel_Frequently\"]]\n",
        "    le_enc = OrdinalEncoder(categories=travel_order,\n",
        "                            handle_unknown=\"use_encoded_value\",\n",
        "                            unknown_value=-1)\n",
        "    # Fit only on training data to prevent data leakage\n",
        "    le_enc.fit(X_train[[\"BusinessTravel\"]])\n",
        "\n",
        "    # Transform both train and test\n",
        "    X_train[\"BusinessTravel_label\"] = le_enc.transform(\n",
        "        X_train[[\"BusinessTravel\"]])\n",
        "    X_test[\"BusinessTravel_label\"] = le_enc.transform(X_test[[\"BusinessTravel\"]])\n",
        "\n",
        "  # 4.2 One-Hot Encoding\n",
        "  # One-Hot keeps each category as a separate feature. Department and MaritalStatus are unordered categorical variables\n",
        "  onehot_features = [\"Department\", \"MaritalStatus\"]\n",
        "  if onehot_features:\n",
        "    onehot_enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop=None)\n",
        "\n",
        "    onehot_enc.fit(X_train[onehot_features])\n",
        "    # Transform both sets\n",
        "    train_oh = onehot_enc.transform(X_train[onehot_features])\n",
        "    test_oh = onehot_enc.transform(X_test[onehot_features])\n",
        "    # Get feature names for one-hot encoded columns\n",
        "    oh_cols = onehot_enc.get_feature_names_out(onehot_features)\n",
        "\n",
        "    # Convert numpy arrays to DataFrame, keep original index\n",
        "    X_train_oh = pd.DataFrame(train_oh, columns=oh_cols, index=X_train.index)\n",
        "    X_test_oh = pd.DataFrame(test_oh, columns=oh_cols, index=X_test.index)\n",
        "    # Concatenate one-hot encoded columns with original DataFrame\n",
        "    X_train = pd.concat([X_train, X_train_oh], axis=1)\n",
        "    X_test = pd.concat([X_test, X_test_oh], axis=1)\n",
        "\n",
        "  # 4.3 Frequency Encoding\n",
        "  # EducationField and JobRole may have many categories, use frequency as replacement\n",
        "  if 'EducationField' in X_train.columns:\n",
        "    freq_enc = X_train[\"EducationField\"].value_counts(normalize=True)\n",
        "\n",
        "    X_train[\"EducationField_freq\"] = X_train[\"EducationField\"].map(freq_enc)\n",
        "    X_test[\"EducationField_freq\"] = X_test[\"EducationField\"].map(\n",
        "        freq_enc).fillna(0)\n",
        "\n",
        "  if 'JobRole' in X_train.columns:\n",
        "    jobrole_freq = X_train[\"JobRole\"].value_counts(normalize=True)\n",
        "\n",
        "    X_train[\"JobRole_freq\"] = X_train[\"JobRole\"].map(jobrole_freq)\n",
        "    X_test[\"JobRole_freq\"] = X_test[\"JobRole\"].map(jobrole_freq).fillna(0)\n",
        "\n",
        "  # 4.4 Binary Encoding\n",
        "  # Gender and OverTime are binary, map to 0/1 instead of strings\n",
        "  if 'Gender' in X_train.columns:\n",
        "    gender_binary_map = {\"Male\": 1, \"Female\": 0}\n",
        "\n",
        "    X_train[\"Gender_bin\"] = X_train[\"Gender\"].map(gender_binary_map)\n",
        "    X_test[\"Gender_bin\"] = X_test[\"Gender\"].map(gender_binary_map)\n",
        "\n",
        "  if 'OverTime' in X_train.columns:\n",
        "    overtime_binary_map = {\"Yes\": 1, \"No\": 0}\n",
        "\n",
        "    X_train[\"OverTime_bin\"] = X_train[\"OverTime\"].map(overtime_binary_map)\n",
        "    X_test[\"OverTime_bin\"] = X_test[\"OverTime\"].map(overtime_binary_map)\n",
        "\n",
        "  # 4.5 Encode Target Variable\n",
        "\n",
        "  # Label the Attrition column, converting 'Yes'/'No' to 1/0\n",
        "  attrition_binary_map = {\"Yes\": 1, \"No\": 0}\n",
        "\n",
        "  y_train = y_train.map(attrition_binary_map)\n",
        "  y_test = y_test.map(attrition_binary_map)\n",
        "\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # 5. Drop original categorical columns.\n",
        "\n",
        "\n",
        "  # Remove original categorical columns after encoding\n",
        "  # Keep only processed encoded features.\n",
        "  columns_to_drop_after_encoding = [\n",
        "      \"BusinessTravel\", \"Department\", \"MaritalStatus\", \"EducationField\",\n",
        "      \"JobRole\", \"Gender\", \"OverTime\"\n",
        "  ]\n",
        "  # Also drop engineered features that were removed in original code\n",
        "  # optional_drops = [\n",
        "  #     'Betweenness_Score', 'Clustering_Coef', 'Neighbor_Risk_Score',\n",
        "  #     'Is_Isolated', 'Tenure_Ratio', 'Income_Per_Age', 'Burnout_Index',\n",
        "  #     'Relative_Sat_to_Dept'\n",
        "  #]\n",
        "  # Drop only if they exist\n",
        "  columns_to_drop_final = [\n",
        "      col for col in columns_to_drop_after_encoding\n",
        "      if col in X_train.columns\n",
        "  ]\n",
        "\n",
        "  X_train = X_train.drop(columns=columns_to_drop_final, errors='ignore')\n",
        "  X_test = X_test.drop(columns=columns_to_drop_final, errors='ignore')\n",
        "\n",
        "  # Check for any remaining missing values\n",
        "  train_missing = X_train.isnull().sum().sum()\n",
        "  test_missing = X_test.isnull().sum().sum()\n",
        "  if train_missing > 0 or test_missing > 0:\n",
        "    print(f\"Warning: Found {train_missing} missing values in train, \"\n",
        "          f\"{test_missing} in test\")\n",
        "  else:\n",
        "    print(\"No missing values detected\")\n",
        "\n",
        "  # Return processed train/test features and target\n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = advanced_data_processing(df_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80TOfCWHFaBJ"
      },
      "source": [
        "### SMOTE\n",
        "\n",
        "**SMOTE (Synthetic Minority Over-sampling Technique)** is a technique used to address class imbalance in datasets, especially for classification problems where one class (minority class) has far fewer samples than the other (majority class).\n",
        "\n",
        "**Key Points / How it Works:**\n",
        "\n",
        "* Generate Synthetic Samples:\n",
        "  * Instead of simply duplicating minority class samples, SMOTE creates new synthetic samples by interpolating between existing minority class points.\n",
        "  * This helps the model learn a more general decision boundary.\n",
        "\n",
        "* Nearest Neighbors:\n",
        "  * For each minority sample, SMOTE selects k nearest neighbors in feature space.\n",
        "  * New samples are generated along the line connecting the sample to its neighbors.\n",
        "\n",
        "* Balance the Dataset:\n",
        "  * After SMOTE, the dataset has a roughly equal number of samples in each class, reducing bias toward the majority class.\n",
        "\n",
        "* Improves Model Performance:\n",
        "  * Helps classifiers (like logistic regression, decision trees, XGBoost) better recognize minority class patterns.\n",
        "  * Reduces false negatives for rare events.\n",
        "\n",
        "**Caveats:**\n",
        "* SMOTE can introduce noise if minority class has outliers.\n",
        "* Best used after train/test split to prevent data leakage.\n",
        "\n",
        "**Summary:**\n",
        "* SMOTE synthetically balances the dataset, creating new minority samples based on neighbors, improving classifier performance for imbalanced datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooX9AUqEFaBJ"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "def advanced_data_smote(X_train, y_train):\n",
        "  \"\"\"\n",
        "  SMOTE-based Class Imbalance Handling and Visualization\n",
        "\n",
        "  This function addresses the severe class imbalance problem in the employee attrition dataset by combining exploratory visualization, SMOTE resampling, and PCA-based distribution analysis.\n",
        "\n",
        "  Steps:\n",
        "    1. Apply SMOTE to generate synthetic samples for minority class\n",
        "    2. PCA visualization for distribution check before and after SMOTE\n",
        "  \"\"\"\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # 1. Check class distribution before SMOTE\n",
        "  print(\n",
        "      f\"Training set distribution before SMOTE: {np.bincount(y_train)}\"\n",
        "  )  # np.bincount counts the number of occurrences of each integer class label\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # 2. Initialize SMOTE\n",
        "  smote = SMOTE(\n",
        "      random_state=42,  # random_state=42: ensure reproducibility\n",
        "      k_neighbors=\n",
        "      3  # k_neighbors=3: number of nearest neighbors used for synthetic sample generation\n",
        "  )\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # 3. Apply SMOTE to handle class imbalance\n",
        "  X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "  # Check class distribution after SMOTE\n",
        "  print(f\"Training set distribution after SMOTE: {np.bincount(y_resampled)}\")\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # 4. PCA visualization for distribution check\n",
        "  # Reduce features to 2 dimensions for scatter plot\n",
        "  pca = PCA(n_components=2)\n",
        "  pca.fit(X_train)  # Fit PCA on original training set only\n",
        "  X_vis_before = pca.transform(X_train)  # Transform original imbalanced data\n",
        "  X_vis_after = pca.transform(X_resampled)  # Transform SMOTE-balanced data\n",
        "\n",
        "  # ---------------------------------------------------------\n",
        "  # 5. Create side-by-side scatter plots\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
        "  # Left plot: before SMOTE\n",
        "  sns.scatterplot(x=X_vis_before[:, 0],\n",
        "                  y=X_vis_before[:, 1],\n",
        "                  hue=y_train,\n",
        "                  ax=ax1,\n",
        "                  palette='viridis',\n",
        "                  alpha=0.6)\n",
        "  ax1.set_title(\"PCA: Before SMOTE (Imbalanced)\")\n",
        "  ax1.legend(title='Class',\n",
        "             loc='upper right',\n",
        "             frameon=True,\n",
        "             facecolor='white',\n",
        "             edgecolor='black',\n",
        "             framealpha=1.0)\n",
        "\n",
        "  # Right plot: after SMOTE\n",
        "  sns.scatterplot(x=X_vis_after[:, 0],\n",
        "                  y=X_vis_after[:, 1],\n",
        "                  hue=y_resampled,\n",
        "                  ax=ax2,\n",
        "                  palette='viridis',\n",
        "                  alpha=0.4)\n",
        "  ax2.set_title(\"PCA: After SMOTE (Balanced)\")\n",
        "  ax2.legend(title='Class',\n",
        "             loc='upper right',\n",
        "             frameon=True,\n",
        "             facecolor='white',\n",
        "             edgecolor='black',\n",
        "             framealpha=1.0)\n",
        "\n",
        "  # Show plots\n",
        "  plt.show()\n",
        "\n",
        "  # Return resampled dataset\n",
        "  return X_resampled, y_resampled\n",
        "\n",
        "\n",
        "X_train, y_train = advanced_data_smote(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT3LEUcaFaBJ"
      },
      "source": [
        "## Module 2: Model Construction\n",
        "\n",
        "**Objective** Define and explore the hyperparameter space to construct a robust and effective model.  \n",
        "\n",
        "**Output** Includes the best hyperparameters found and the corresponding validation performance, which can be used to train the final model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbxBwZirFaBK"
      },
      "source": [
        "### Hyperparameter Optimization for XGBoost using Optuna\n",
        "\n",
        "In binary classification problems, XGBoost is a powerful non-linear model that often achieves high predictive performance. However, its performance is highly sensitive to hyperparameters such as tree depth, learning rate, number of trees, gamma, and subsample ratios.  \n",
        "\n",
        "Manual tuning of these parameters can be time-consuming, inefficient, and prone to suboptimal results.\n",
        "To address this, project uses **Optuna**, an automated hyperparameter optimization framework that efficiently searches for the best combination of parameters.\n",
        "\n",
        "* Systematic exploration of the hyperparameter space.\n",
        "* Robust model evaluation using Stratified K-Fold cross-validation to maintain class balance in each fold.\n",
        "* Optimization of a meaningful performance metric, in this case, **AUC (Area Under the ROC Curve)**.\n",
        "* Reduction of human bias and trial-and-error in hyperparameter selection, leading to improved model performance and stability.\n",
        "\n",
        "**Workflow**\n",
        "  1. Run Optuna optimization to explore hyperparameter space\n",
        "  2. Extract trial history (learning_rate, max_depth, AUC)\n",
        "  3. Create heatmap showing performance landscape\n",
        "  4. Identify optimal parameter region visually\n",
        "\n",
        "**Key Hyperparameters**\n",
        "* **Learning Rate**: Step size at each iteration\n",
        "* **Max Depth**: Maximum tree depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9dKI7ZaFaBK"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter Tuning Based on Optuna Optimization\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import warnings\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set professional visualization style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Optuna Hyperparameter Optimization\n",
        "\n",
        "\n",
        "def objective(trial, X_train, y_train):\n",
        "  \"\"\"\n",
        "  XGBoost Hyperparameter Optimization with Optuna\n",
        "\n",
        "  This script performs automated hyperparameter tuning for an XGBoost binary classification model using Optuna. It evaluates model performance using Stratified K-Fold cross-validation and optimizes the area under the ROC curve (AUC).\n",
        "\n",
        "  Steps:\n",
        "    1. Define the hyperparameter search space for XGBoost, including tree depth, learning rate, number of trees, gamma, subsample ratio, and feature subsample ratio.\n",
        "    2. Initialize the XGBoost classifier with trial-suggested parameters.\n",
        "    3. Perform 5-fold Stratified K-Fold cross-validation to ensure balanced class distributions in each fold.\n",
        "    4. Calculate the AUC for each fold and return 1 - mean(AUC) as the objective function for Optuna to minimize.\n",
        "    5. Run Optuna study to search for the optimal hyperparameters.\n",
        "    6. Output the best hyperparameter combination and the corresponding AUC score.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define hyperparameter search space\n",
        "  param = {\n",
        "      'objective':\n",
        "      'binary:logistic',\n",
        "      'eval_metric':\n",
        "      'auc',\n",
        "      'max_depth':\n",
        "      trial.suggest_int('max_depth', 3, 20),\n",
        "      'learning_rate':\n",
        "      trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "      'n_estimators':\n",
        "      trial.suggest_int('n_estimators', 100, 1000),\n",
        "      'gamma':\n",
        "      trial.suggest_float('gamma', 1e-8, 1.0),\n",
        "      'subsample':\n",
        "      trial.suggest_float('subsample', 0.6, 1.0, step=0.1),\n",
        "      'colsample_bytree':\n",
        "      trial.suggest_float('colsample_bytree', 0.6, 1.0, step=0.1),\n",
        "      'random_state':\n",
        "      7,\n",
        "      'use_label_encoder':\n",
        "      False,\n",
        "      'n_jobs':\n",
        "      -1,\n",
        "      'verbosity':\n",
        "      0\n",
        "  }\n",
        "\n",
        "  # Initialize XGBoost classifier\n",
        "  xgb = XGBClassifier(**param)\n",
        "\n",
        "  # Stratified K-Fold cross-validation\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
        "\n",
        "  # Evaluate using cross_val_score with AUC\n",
        "  scores = cross_val_score(xgb,\n",
        "                           X_train,\n",
        "                           y_train,\n",
        "                           cv=cv,\n",
        "                           scoring='roc_auc',\n",
        "                           n_jobs=-1)\n",
        "\n",
        "  # Return 1 - mean(AUC) as objective to minimize\n",
        "  return 1 - scores.mean()\n",
        "\n",
        "\n",
        "def run_optuna_optimization(X_train,\n",
        "                            y_train,\n",
        "                            n_trials=100,\n",
        "                            show_progress=True):\n",
        "  \"\"\"\n",
        "    Run Optuna hyperparameter optimization.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : array-like\n",
        "        Training features\n",
        "    y_train : array-like\n",
        "        Training target\n",
        "    n_trials : int\n",
        "        Number of optimization trials\n",
        "    show_progress : bool\n",
        "        Whether to show progress bar\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    optuna.Study : Completed Optuna study object\n",
        "    \"\"\"\n",
        "\n",
        "  print(\"OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
        "  print(f\"\\nConfiguration:\")\n",
        "  print(f\"  Number of trials: {n_trials}\")\n",
        "  print(f\"  Cross-validation: 5-fold Stratified\")\n",
        "  print(f\"  Objective: Maximize AUC\")\n",
        "  print(f\"  Search space:\")\n",
        "  print(f\"    - Learning Rate: [0.01, 0.30]\")\n",
        "  print(f\"    - Max Depth: [3, 20]\")\n",
        "  print(f\"    - N Estimators: [100, 1000]\")\n",
        "  print(f\"    - Gamma: [1e-8, 1.0]\")\n",
        "  print(f\"    - Subsample: [0.6, 1.0]\")\n",
        "  print(f\"    - Colsample by Tree: [0.6, 1.0]\")\n",
        "  print(\"\\n\" + \"-\" * 70)\n",
        "  print(\"Starting optimization...\\n\")\n",
        "\n",
        "  # Create Optuna study\n",
        "  study = optuna.create_study(direction='minimize',\n",
        "                              study_name='xgboost_optimization')\n",
        "\n",
        "  # Run optimization with progress bar\n",
        "  study.optimize(lambda trial: objective(trial, X_train, y_train),\n",
        "                 n_trials=n_trials,\n",
        "                 show_progress_bar=show_progress)\n",
        "\n",
        "  # Output best parameters\n",
        "  best_params = study.best_params\n",
        "  best_score = 1 - study.best_value\n",
        "\n",
        "  print(\"-\" * 70)\n",
        "  print(\"OPTIMIZATION COMPLETED\")\n",
        "  print(\"\\nBest Hyperparameters Found:\")\n",
        "  for key, value in best_params.items():\n",
        "    if isinstance(value, float):\n",
        "      print(f\"  {key:20s}: {value:.6f}\")\n",
        "    else:\n",
        "      print(f\"  {key:20s}: {value}\")\n",
        "  print()\n",
        "  print(f\"  Best AUC Score:     {best_score:.6f}\")\n",
        "  print(\"-\" * 70)\n",
        "\n",
        "  return study\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Extract Trial Data for Visualization\n",
        "\n",
        "\n",
        "def extract_trial_data(study):\n",
        "  \"\"\"\n",
        "  Extract learning_rate, max_depth, and AUC from Optuna trials.\n",
        "  \"\"\"\n",
        "\n",
        "  trials_data = []\n",
        "\n",
        "  for trial in study.trials:\n",
        "    # Only include completed trials\n",
        "    if trial.state == optuna.trial.TrialState.COMPLETE:\n",
        "      trials_data.append({\n",
        "          'trial_number':\n",
        "          trial.number,\n",
        "          'learning_rate':\n",
        "          trial.params.get('learning_rate'),\n",
        "          'max_depth':\n",
        "          trial.params.get('max_depth'),\n",
        "          'n_estimators':\n",
        "          trial.params.get('n_estimators'),\n",
        "          'gamma':\n",
        "          trial.params.get('gamma'),\n",
        "          'subsample':\n",
        "          trial.params.get('subsample'),\n",
        "          'colsample_bytree':\n",
        "          trial.params.get('colsample_bytree'),\n",
        "          'auc':\n",
        "          1 - trial.value  # Convert back to AUC\n",
        "      })\n",
        "\n",
        "  df = pd.DataFrame(trials_data)\n",
        "\n",
        "  print(f\"Extracted {len(df)} completed trials\")\n",
        "  print(f\"AUC range: [{df['auc'].min():.4f}, {df['auc'].max():.4f}]\")\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def optuna_hperparameter_optimization(X_train, y_train, n_trials=100):\n",
        "  # Step 1: Run Optuna optimization\n",
        "  study = run_optuna_optimization(X_train, y_train, n_trials=n_trials)\n",
        "\n",
        "  # Step 2: Extract trial data\n",
        "  trials_df = extract_trial_data(study)\n",
        "\n",
        "  return study, trials_df\n",
        "\n",
        "\n",
        "study, trials_df = optuna_hperparameter_optimization(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOZzdM-tFaBK"
      },
      "source": [
        "### Hyperparameter Optimization Analysis and Visualization\n",
        "\n",
        "Provides a comprehensive post-hoc analysis and visualization framework for hyperparameter optimization conducted with Optuna.\n",
        "It aims to systematically examine both the global structure of the hyperparameter search space and the local behavior of individual optimization trials.\n",
        "\n",
        "* A two-dimensional heatmap is constructed to illustrate the relationship between the learning rate, maximum tree depth, and the cross-validated AUC score.\n",
        "\n",
        "  The hyperparameter space is discretized into bins, and the mean AUC within each bin is computed, enabling the identification of parameter regions that exhibit consistently strong performance rather than relying on isolated optimal trials.\n",
        "\n",
        "  The best-performing Optuna trial is explicitly highlighted to facilitate direct comparison between region-level trends and point-wise optimality.\n",
        "\n",
        "* A smooth performance landscape is generated using scattered trial results and cubic interpolation.\n",
        "  This contour-based visualization captures the continuous variation of model performance across the hyperparameter space, providing an intuitive understanding of sensitivity and interaction effects between learning rate and tree depth.\n",
        "  The actual Optuna trials and the globally optimal configuration are overlaid to preserve the connection between interpolated estimates and observed evaluations.\n",
        "\n",
        "* The optimization dynamics are analyzed by visualizing the evolution of AUC scores across trials and estimating the relative importance of individual hyperparameters.\n",
        "  The optimization history reveals convergence behavior and performance stability, while the parameter importance analysis offers insights into which hyperparameters most strongly influence model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-L8pSMrFaBK"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter Optimization Analysis and Visualization\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Create Hyperparameter Heatmap\n",
        "\n",
        "\n",
        "def plot_hyperparameter_heatmap(trials_df, study, figsize=(14, 10)):\n",
        "  \"\"\"\n",
        "  Create heatmap showing relationship between learning_rate, max_depth, and AUC.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create grid for heatmap\n",
        "  lr_bins = np.linspace(trials_df['learning_rate'].min(),\n",
        "                        trials_df['learning_rate'].max(), 12)\n",
        "  depth_bins = np.arange(trials_df['max_depth'].min(),\n",
        "                         trials_df['max_depth'].max() + 2, 2)\n",
        "\n",
        "  # Bin the data and compute mean AUC for each cell\n",
        "  trials_df['lr_bin'] = pd.cut(trials_df['learning_rate'], bins=lr_bins)\n",
        "  trials_df['depth_bin'] = pd.cut(trials_df['max_depth'], bins=depth_bins)\n",
        "\n",
        "  # Aggregate by taking mean AUC in each bin\n",
        "  heatmap_data = trials_df.groupby(['depth_bin',\n",
        "                                    'lr_bin'])['auc'].mean().unstack()\n",
        "\n",
        "  # Get bin centers for labels\n",
        "  lr_labels = [(interval.left + interval.right) / 2\n",
        "               for interval in heatmap_data.columns]\n",
        "  depth_labels = [(interval.left + interval.right) / 2\n",
        "                  for interval in heatmap_data.index]\n",
        "\n",
        "  # Create figure\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  # Create heatmap\n",
        "  sns.heatmap(heatmap_data,\n",
        "              annot=True,\n",
        "              fmt='.4f',\n",
        "              cmap='RdYlGn',\n",
        "              cbar_kws={\n",
        "                  'label': 'Cross-Validated AUC Score',\n",
        "                  'shrink': 0.8\n",
        "              },\n",
        "              linewidths=0.5,\n",
        "              linecolor='white',\n",
        "              square=False,\n",
        "              vmin=trials_df['auc'].min(),\n",
        "              vmax=trials_df['auc'].max(),\n",
        "              ax=ax,\n",
        "              annot_kws={\n",
        "                  'size': 9,\n",
        "                  'weight': 'bold'\n",
        "              })\n",
        "\n",
        "  # Customize labels\n",
        "  ax.set_xticklabels([f'{x:.3f}' for x in lr_labels], rotation=45, ha='right')\n",
        "  ax.set_yticklabels([f'{int(y)}' for y in depth_labels], rotation=0)\n",
        "\n",
        "  ax.set_xlabel('Learning Rate', fontsize=14, fontweight='bold', labelpad=10)\n",
        "  ax.set_ylabel('Max Depth', fontsize=14, fontweight='bold', labelpad=10)\n",
        "  ax.set_title(\n",
        "      f'XGBoost Hyperparameter Optimization Heatmap\\n'\n",
        "      f'Optuna Trials: {len(trials_df)} | Best AUC: {trials_df[\"auc\"].max():.4f}',\n",
        "      fontsize=16,\n",
        "      fontweight='bold',\n",
        "      pad=20)\n",
        "\n",
        "  # Mark best trial\n",
        "  best_trial = study.best_trial\n",
        "  best_lr = best_trial.params['learning_rate']\n",
        "  best_depth = best_trial.params['max_depth']\n",
        "\n",
        "  # Find closest bin position\n",
        "  lr_bin_idx = np.digitize(best_lr, lr_bins) - 1\n",
        "  depth_bin_idx = np.digitize(best_depth, depth_bins) - 1\n",
        "\n",
        "  # Add star marker\n",
        "  ax.scatter(lr_bin_idx + 0.5,\n",
        "             depth_bin_idx + 0.8,\n",
        "             marker='*',\n",
        "             s=300,\n",
        "             c='blue',\n",
        "             edgecolors='white',\n",
        "             linewidths=1,\n",
        "             label=f'Best: LR={best_lr:.4f}, Depth={best_depth}',\n",
        "             zorder=10)\n",
        "\n",
        "  ax.legend(loc='upper right',\n",
        "            frameon=True,\n",
        "            facecolor='white',\n",
        "            edgecolor='white',\n",
        "            framealpha=0.9)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return fig\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Create Scatter Plot with Interpolation\n",
        "\n",
        "\n",
        "def plot_scatter_with_interpolation(trials_df, study, figsize=(14, 10)):\n",
        "  \"\"\"\n",
        "  Create interpolated contour plot showing smooth performance landscape.\n",
        "  \"\"\"\n",
        "\n",
        "  # Extract data\n",
        "  lr = trials_df['learning_rate'].values\n",
        "  depth = trials_df['max_depth'].values\n",
        "  auc = trials_df['auc'].values\n",
        "\n",
        "  # Create fine grid for interpolation\n",
        "  lr_grid = np.linspace(lr.min(), lr.max(), 100)\n",
        "  depth_grid = np.linspace(depth.min(), depth.max(), 100)\n",
        "  lr_mesh, depth_mesh = np.meshgrid(lr_grid, depth_grid)\n",
        "\n",
        "  # Interpolate AUC values\n",
        "  auc_mesh = griddata((lr, depth),\n",
        "                      auc, (lr_mesh, depth_mesh),\n",
        "                      method='cubic',\n",
        "                      fill_value=auc.mean())\n",
        "\n",
        "  # Create figure\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  # Create filled contour\n",
        "  contour_filled = ax.contourf(lr_mesh,\n",
        "                               depth_mesh,\n",
        "                               auc_mesh,\n",
        "                               levels=20,\n",
        "                               cmap='RdYlGn',\n",
        "                               alpha=0.8)\n",
        "\n",
        "  # Add contour lines\n",
        "  contour_lines = ax.contour(lr_mesh,\n",
        "                             depth_mesh,\n",
        "                             auc_mesh,\n",
        "                             levels=10,\n",
        "                             colors='black',\n",
        "                             linewidths=0.5,\n",
        "                             alpha=0.4)\n",
        "\n",
        "  # Add labels to contour lines\n",
        "  ax.clabel(contour_lines, inline=True, fontsize=8, fmt='%.3f')\n",
        "\n",
        "  # Overlay actual trial points\n",
        "  scatter = ax.scatter(lr,\n",
        "                       depth,\n",
        "                       c=auc,\n",
        "                       cmap='RdYlGn',\n",
        "                       s=50,\n",
        "                       alpha=0.6,\n",
        "                       edgecolors='black',\n",
        "                       linewidths=0.5,\n",
        "                       label='Optuna Trials')\n",
        "\n",
        "  # Mark best trial\n",
        "  best_trial = study.best_trial\n",
        "  best_lr = best_trial.params['learning_rate']\n",
        "  best_depth = best_trial.params['max_depth']\n",
        "  best_auc = 1 - best_trial.value\n",
        "\n",
        "  ax.scatter(\n",
        "      best_lr,\n",
        "      best_depth,\n",
        "      marker='*',\n",
        "      s=300,\n",
        "      c='blue',\n",
        "      edgecolors='white',\n",
        "      linewidths=1,\n",
        "      label=f'Best: LR={best_lr:.4f}, Depth={best_depth}, AUC={best_auc:.4f}',\n",
        "      zorder=10)\n",
        "\n",
        "  # Add colorbar\n",
        "  cbar = plt.colorbar(contour_filled, ax=ax, label='AUC Score')\n",
        "  cbar.ax.tick_params(labelsize=10)\n",
        "\n",
        "  # Customize plot\n",
        "  ax.set_xlabel('Learning Rate', fontsize=13, fontweight='bold')\n",
        "  ax.set_ylabel('Max Depth', fontsize=13, fontweight='bold')\n",
        "  ax.set_title(\n",
        "      f'XGBoost Hyperparameter Performance Landscape\\n'\n",
        "      f'Interpolated from {len(trials_df)} Optuna Trials',\n",
        "      fontsize=15,\n",
        "      fontweight='bold',\n",
        "      pad=15)\n",
        "  ax.legend(loc='upper left', fontsize=11, framealpha=0.95)\n",
        "  ax.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return fig\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Optimization History Visualization\n",
        "\n",
        "\n",
        "def plot_optimization_history(study, figsize=(14, 5)):\n",
        "  \"\"\"\n",
        "  Visualize optimization progress over trials.\n",
        "  \"\"\"\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "  # Left plot: AUC over trials\n",
        "  trials = [\n",
        "      t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE\n",
        "  ]\n",
        "  trial_numbers = [t.number for t in trials]\n",
        "  auc_values = [1 - t.value for t in trials]\n",
        "\n",
        "  ax1.plot(trial_numbers,\n",
        "           auc_values,\n",
        "           'o-',\n",
        "           color='#2E86AB',\n",
        "           alpha=0.6,\n",
        "           linewidth=1.5)\n",
        "  ax1.axhline(y=max(auc_values),\n",
        "              color='#E63946',\n",
        "              linestyle='--',\n",
        "              linewidth=2,\n",
        "              label=f'Best AUC: {max(auc_values):.4f}')\n",
        "  ax1.fill_between(trial_numbers, auc_values, alpha=0.2, color='#2E86AB')\n",
        "\n",
        "  ax1.set_xlabel('Trial Number', fontsize=12, fontweight='bold')\n",
        "  ax1.set_ylabel('AUC Score', fontsize=12, fontweight='bold')\n",
        "  ax1.set_title('Optimization Progress',\n",
        "                fontsize=14,\n",
        "                fontweight='bold',\n",
        "                pad=15)\n",
        "  ax1.legend(fontsize=10)\n",
        "  ax1.grid(True, alpha=0.3)\n",
        "\n",
        "  # Right plot: Parameter importance\n",
        "  try:\n",
        "    importances = optuna.importance.get_param_importances(study)\n",
        "    params = list(importances.keys())[:6]  # Top 6\n",
        "    values = [importances[p] for p in params]\n",
        "\n",
        "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(params)))\n",
        "    bars = ax2.barh(params, values, color=colors, alpha=0.8, edgecolor='black')\n",
        "\n",
        "    for bar, value in zip(bars, values):\n",
        "      ax2.text(value + 0.01,\n",
        "               bar.get_y() + bar.get_height() / 2,\n",
        "               f'{value:.3f}',\n",
        "               va='center',\n",
        "               fontweight='bold')\n",
        "\n",
        "    ax2.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Hyperparameter', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('Hyperparameter Importance',\n",
        "                  fontsize=14,\n",
        "                  fontweight='bold',\n",
        "                  pad=15)\n",
        "    ax2.invert_yaxis()\n",
        "    ax2.grid(axis='x', alpha=0.3)\n",
        "  except:\n",
        "    ax2.text(0.5,\n",
        "             0.5,\n",
        "             'Importance analysis requires\\nmore diverse trials',\n",
        "             ha='center',\n",
        "             va='center',\n",
        "             fontsize=12)\n",
        "    ax2.set_xlim([0, 1])\n",
        "    ax2.set_ylim([0, 1])\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Main Execution Function\n",
        "\n",
        "\n",
        "def optimization_analysis(study, trials_df):\n",
        "  \"\"\"\n",
        "  Execute complete Optuna optimization with comprehensive visualizations.\n",
        "  \"\"\"\n",
        "\n",
        "  # Step 1: Create heatmap\n",
        "  print(\"\\nCreating hyperparameter heatmap...\")\n",
        "  plot_hyperparameter_heatmap(trials_df, study)\n",
        "\n",
        "  # Step 2: Create contour plot\n",
        "  print(\"\\nCreating performance landscape...\")\n",
        "  plot_scatter_with_interpolation(trials_df, study)\n",
        "\n",
        "  # Step 3: Show optimization history\n",
        "  print(\"\\nCreating optimization history...\")\n",
        "  plot_optimization_history(study)\n",
        "\n",
        "\n",
        "optimization_analysis(study, trials_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJv2Nq0yFaBK"
      },
      "source": [
        "**Heatmap Figure** illustrates the relationship between learning rate, maximum tree depth, and the cross-validated AUC score obtained from Optuna-based hyperparameter optimization.\n",
        "\n",
        "Each cell in the heatmap represents the average AUC of all trials whose hyperparameters fall within the corresponding learning rate and depth bins, thereby highlighting regions of consistently strong performance rather than isolated optimal configurations.\n",
        "\n",
        "The blue star marks the best individual Optuna trial, which achieved the highest AUC among all evaluated hyperparameter combinations.\n",
        "Notably, this point does not necessarily coincide with the heatmap cell exhibiting the maximum average AUC, since the latter aggregates performance over multiple trials and may be influenced by suboptimal configurations within the same bin.\n",
        "\n",
        "This discrepancy reflects the inherent difference between point-wise optimality and region-wise robustness.\n",
        "While the best trial indicates the maximum achievable performance, the heatmap provides a more stable and interpretable view of the hyperparameter landscape, emphasizing parameter regions that yield reliably high performance across multiple evaluations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axLGC-YZFaBK"
      },
      "source": [
        "### XGBoost Model Construction\n",
        "\n",
        "* Initialize the XGBoost classifier using the previously optimized hyperparameters and train the model on the validation set.\n",
        "* To ensure that the model is well-tuned, avoids overfitting, and is ready for subsequent performance evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sscdIq3FaBK"
      },
      "outputs": [],
      "source": [
        "# XGBoost Model Construction\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Split the dataset into training and validation sets\n",
        "# Split the dataset into training and validation sets for model training and early stopping\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    test_size=0.2,  # 20% of data used for validation\n",
        "    random_state=7,  # Random seed for reproducibility\n",
        "    stratify=y_train,\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Initialize XGBoost classifier with best-found hyperparameters\n",
        "best_params = study.best_params\n",
        "\n",
        "xgb_model = XGBClassifier(\n",
        "    max_depth=best_params['max_depth'],  # Maximum depth of each tree\n",
        "    learning_rate=best_params['learning_rate'],  # Step size shrinkage\n",
        "    n_estimators=best_params['n_estimators'],  # Number of trees\n",
        "    gamma=best_params['gamma'],  # Minimum loss reduction required for split\n",
        "    subsample=best_params[\n",
        "        'subsample'],  # Subsample ratio of the training instances\n",
        "    colsample_bytree=best_params[\n",
        "        'colsample_bytree'],  # Subsample ratio of columns for each tree\n",
        "    random_state=7,\n",
        "    use_label_encoder=False,  # Disable deprecated label encoder\n",
        "    verbosity=0,  # Silent mode, suppress training messages\n",
        "    n_jobs=-1,  # Use all CPU cores\n",
        "    eval_metric='auc',  # Evaluation metric for validation\n",
        "    verbose=50,  # Print message every 50 trees (if training)\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Fit the model on training set with early stopping on validation set\n",
        "xgb_model.fit(\n",
        "    X_tr,\n",
        "    y_tr,\n",
        "    eval_set=[(X_val, y_val)],  # Validation set for early stopping\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTRhIwDVFaBL"
      },
      "source": [
        "## Module 3: Model Evaluation & Performance\n",
        "\n",
        "**Objective** Assess the trained model’s predictive effectiveness and generalization ability.\n",
        "\n",
        "**Output** Includes classification metrics (Precision, Recall, F1-score), learning curves illustrating training versus validation performance across different sample sizes, ROC curves evaluating discriminative capability, and an analysis of key features that contribute most to the model’s predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmntNfEjFaBL"
      },
      "source": [
        "### XGBoost Model Evaluation and Visualization\n",
        "\n",
        "Evaluate a trained XGBoost classification model and generates two types of visualization charts.\n",
        "\n",
        "* AUC curve during training process\n",
        "  * Calculate AUC and accuracy metrics on validation set\n",
        "  * Visualize AUC trend throughout the training process\n",
        "* Learning curve showing model performance across different training set sizes\n",
        "  * Diagnose overfitting/underfitting through learning curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52I0xd0yFaBL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import learning_curve, StratifiedKFold\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Model Performance Evaluation\n",
        "\n",
        "\n",
        "# Get evaluation results recorded during training. This requires the model to be trained with eval_set parameter\n",
        "evals_result = xgb_model.evals_result()\n",
        "\n",
        "# Make predictions on validation set\n",
        "y_val_pred = xgb_model.predict(X_val)\n",
        "y_val_prob = xgb_model.predict_proba(X_val)[:, 1]  # Returns probabilities for each class\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "# AUC (Area Under ROC Curve): Measures model's ability to distinguish between classes\n",
        "auc = roc_auc_score(y_val, y_val_prob)\n",
        "# Accuracy: Proportion of correctly predicted samples\n",
        "acc = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"Validation AUC: {auc:.4f}\")\n",
        "print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Visualize Training Process\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Extract AUC history from validation set\n",
        "val_auc = evals_result['validation_0']['auc']\n",
        "\n",
        "# Create plot showing how validation AUC changes after each boosting round\n",
        "# Observe how model performance improves with training rounds. Detect potential overfitting (if validation AUC starts to decrease)\n",
        "ax1.plot(val_auc,\n",
        "         color='#2E86AB',\n",
        "         linewidth=2.5,\n",
        "         label='Validation AUC',\n",
        "         alpha=0.9)\n",
        "ax1.axhline(y=max(val_auc),\n",
        "            color='#A23B72',\n",
        "            linestyle='--',\n",
        "            linewidth=1.5,\n",
        "            label=f'Best AUC: {max(val_auc):.4f}',\n",
        "            alpha=0.7)\n",
        "ax1.fill_between(range(len(val_auc)), val_auc, alpha=0.1, color='#2E86AB')\n",
        "\n",
        "ax1.set_xlabel('Boosting Rounds', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('AUC Score', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Validation AUC During Training',\n",
        "              fontsize=14,\n",
        "              fontweight='bold',\n",
        "              pad=15)\n",
        "ax1.legend(loc='lower right', fontsize=11, framealpha=0.9)\n",
        "ax1.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)\n",
        "ax1.set_ylim([min(val_auc) - 0.01, max(val_auc) + 0.01])\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Generate Learning Curve\n",
        "\n",
        "\n",
        "# Learning_curve requires an untrained estimator\n",
        "# Create a new estimator with the same hyperparameters as the original model\n",
        "\n",
        "# Create a new estimator with identical parameters but untrained\n",
        "base_estimator = XGBClassifier(\n",
        "    **xgb_model.get_params()  # Use get_params() to retrieve all hyperparameters from the trained model\n",
        ")\n",
        "\n",
        "# Set up cross-validation strategy\n",
        "cv = StratifiedKFold(  # Maintains the same proportion of positive/negative samples\n",
        "    n_splits=5,  # Split data into 5 folds\n",
        "    shuffle=True,  # Shuffle data before splitting\n",
        "    random_state=7  # Random seed for reproducibility\n",
        ")\n",
        "\n",
        "# Calculate learning curve\n",
        "# learning_curve trains the model on different training set sizes and evaluates performance\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    base_estimator,  # Use untrained estimator\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=cv,  # Cross-validation strategy\n",
        "    scoring='roc_auc',  # Evaluation metric: AUC\n",
        "    train_sizes=np.linspace(\n",
        "        0.1, 1.0,\n",
        "        10),  # Sample 10 points between 10% and 100% of training data\n",
        "    n_jobs=-1,  # Use all available CPU cores for parallel computation\n",
        "    verbose=1,  # Display progress information\n",
        ")\n",
        "\n",
        "# Calculate average scores for each training set size. Calculate mean across each row (each training set size)\n",
        "train_scores_mean = train_scores.mean(axis=1)\n",
        "train_scores_std = train_scores.std(axis=1)\n",
        "val_scores_mean = val_scores.mean(axis=1)\n",
        "val_scores_std = val_scores.std(axis=1)\n",
        "\n",
        "# Plot learning curve\n",
        "ax2.plot(train_sizes,\n",
        "         train_scores_mean,\n",
        "         'o-',\n",
        "         color='#F18F01',\n",
        "         linewidth=2.5,\n",
        "         markersize=6,\n",
        "         label='Training AUC',\n",
        "         alpha=0.9)\n",
        "ax2.plot(train_sizes,\n",
        "         val_scores_mean,\n",
        "         'o-',\n",
        "         color='#2E86AB',\n",
        "         linewidth=2.5,\n",
        "         markersize=6,\n",
        "         label='Validation AUC',\n",
        "         alpha=0.9)\n",
        "\n",
        "# Add standard deviation bands\n",
        "ax2.fill_between(train_sizes,\n",
        "                 train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std,\n",
        "                 alpha=0.15,\n",
        "                 color='#F18F01')\n",
        "ax2.fill_between(train_sizes,\n",
        "                 val_scores_mean - val_scores_std,\n",
        "                 val_scores_mean + val_scores_std,\n",
        "                 alpha=0.15,\n",
        "                 color='#2E86AB')\n",
        "\n",
        "ax2.set_xlabel('Training Set Size', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('AUC Score', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Learning Curve Analysis',\n",
        "              fontsize=14,\n",
        "              fontweight='bold',\n",
        "              pad=15)\n",
        "ax2.legend(loc='lower right', fontsize=11, framealpha=0.9)\n",
        "ax2.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)\n",
        "ax2.set_ylim([0.5, 1.05])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2AZu9kzFaBL"
      },
      "source": [
        "### Interpret Learning Curves\n",
        "* Both training and validation scores are low: **Underfitting**; Solution: Increase model complexity, add more features\n",
        "\n",
        "* High training score but low validation score with large gap: **Overfitting**; Solution: Add more training data, use regularization, reduce model complexity\n",
        "\n",
        "* Both curves converge and plateau: **Model has learned sufficiently**; If performance is still unsatisfactory, consider better feature engineering or trying different models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT-1K_WMFaBL"
      },
      "source": [
        "### Model Prediction and Comprehensive Evaluation\n",
        "* Precision: When false positives are costly (e.g., spam detection)\n",
        "* Recall: When false negatives are costly (e.g., disease diagnosis)\n",
        "* F1-Score: When you need balance between precision and recall\n",
        "* AUC: Overall model performance across all thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK1sAbCiFaBL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (roc_auc_score, accuracy_score,\n",
        "                             classification_report, confusion_matrix)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Generate Predictions on Test Set\n",
        "\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# predict_proba(): Returns probability estimates for each class\n",
        "# [:, 1] extracts the probability of the positive class (class 1)\n",
        "y_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Calculate Core Performance Metrics\n",
        "\n",
        "\n",
        "# Calculate AUC (Area Under ROC Curve)\n",
        "# AUC measures the model's ability to distinguish between classes\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Calculate Accuracy\n",
        "# Accuracy is the proportion of correct predictions (both positive and negative)\n",
        "# Formula: (TP + TN) / (TP + TN + FP + FN). Can be misleading for imbalanced datasets\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Display Results with Professional Formatting\n",
        "\n",
        "\n",
        "# Print header with visual separation\n",
        "print(\"MODEL EVALUATION RESULTS\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Display primary metrics\n",
        "print(f\"Primary Performance Metrics:\")\n",
        "print(f\"   Test AUC:      {auc:.4f}\")\n",
        "print(f\"   Test Accuracy: {acc:.4f}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Generate and display detailed classification report\n",
        "# Classification report includes:\n",
        "#   - Precision: TP / (TP + FP) - How many selected items are relevant\n",
        "#   - Recall: TP / (TP + FN) - How many relevant items are selected\n",
        "#   - F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
        "#   - Support: Number of actual occurrences of each class\n",
        "print(\"Detailed Classification Report:\")\n",
        "print(\n",
        "    classification_report(\n",
        "        y_test,\n",
        "        y_pred,\n",
        "        target_names=['Class 0 (Negative)', 'Class 1 (Positive)'],\n",
        "        digits=4))\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Confusion Matrix Analysis\n",
        "\n",
        "\n",
        "# Calculate confusion matrix\n",
        "#              Predicted\n",
        "#              0      1\n",
        "# Actual  0   TN     FP\n",
        "#         1   FN     TP\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"-\" * 70)\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display confusion matrix components\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"\\nConfusion Matrix Interpretation:\")\n",
        "print(f\"   True Negatives (TN):  {tn:>6}  (Correctly predicted negative)\")\n",
        "print(f\"   False Positives (FP): {fp:>6}  (Incorrectly predicted positive)\")\n",
        "print(f\"   False Negatives (FN): {fn:>6}  (Incorrectly predicted negative)\")\n",
        "print(f\"   True Positives (TP):  {tp:>6}  (Correctly predicted positive)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyHMvLW-FaBL"
      },
      "source": [
        "### ROC and Precision-Recall Curves Visualization\n",
        "* ROC Curve\n",
        "  * Shows trade-off between True Positive Rate and False Positive Rate.\n",
        "  * Good for balanced datasets, overall model performance assessment\n",
        "* PR Curve\n",
        "  * Shows trade-off between Precision and Recall.\n",
        "  * Better for imbalanced datasets, focuses on positive class performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjrK120rFaBL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
        "\n",
        "\n",
        "# Set global matplotlib style for consistent, professional appearance\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. ROC and Precision-Recall Curves\n",
        "\n",
        "\n",
        "# Calculate ROC curve coordinates\n",
        "# fpr: False Positive Rate at different thresholds\n",
        "# tpr: True Positive Rate (Recall/Sensitivity) at different thresholds\n",
        "# thresholds: Decision threshold values used to generate fpr and tpr\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "# Calculate Area Under ROC Curve (AUC-ROC)\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Calculate Precision-Recall curve coordinates\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Calculate Average Precision (AP) score\n",
        "# AP summarizes the PR curve as weighted mean of precisions. Equivalent to area under the PR curve\n",
        "average_precision = average_precision_score(y_test, y_prob)\n",
        "\n",
        "# Calculate the proportion of positive samples for PR baseline\n",
        "positive_ratio = y_test.sum() / len(y_test)\n",
        "\n",
        "# Create figure with two subplots side by side\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. ROC Curve\n",
        "\n",
        "\n",
        "# Plot the ROC curve with professional styling\n",
        "ax1.plot(\n",
        "    fpr,\n",
        "    tpr,\n",
        "    color='#2E86AB',  # Deep blue color\n",
        "    linewidth=2.5,\n",
        "    label=f'ROC Curve (AUC = {roc_auc:.4f})',\n",
        "    alpha=0.9)\n",
        "\n",
        "# Plot diagonal reference line (random classifier baseline)\n",
        "ax1.plot(\n",
        "    [0, 1],\n",
        "    [0, 1],\n",
        "    color='#A23B72',  # Contrasting color for reference line\n",
        "    linestyle='--',\n",
        "    linewidth=2,\n",
        "    label='Random Classifier (AUC = 0.50)',\n",
        "    alpha=0.7)\n",
        "\n",
        "# Customize ROC plot appearance\n",
        "ax1.set_xlabel('False Positive Rate (1 - Specificity)',\n",
        "               fontsize=12,\n",
        "               fontweight='bold')\n",
        "ax1.set_ylabel('True Positive Rate (Sensitivity/Recall)',\n",
        "               fontsize=12,\n",
        "               fontweight='bold')\n",
        "ax1.set_title('Receiver Operating Characteristic (ROC) Curve',\n",
        "              fontsize=14,\n",
        "              fontweight='bold',\n",
        "              pad=20)\n",
        "ax1.legend(loc='lower right', fontsize=11, framealpha=0.9)\n",
        "ax1.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)\n",
        "ax1.set_xlim([0.0, 1.0])\n",
        "ax1.set_ylim([0.0, 1.05])\n",
        "\n",
        "# Add subtle background color to emphasize good performance region\n",
        "ax1.fill_between(fpr, tpr, alpha=0.1, color='#2E86AB')\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Precision-Recall Curve\n",
        "\n",
        "\n",
        "# Plot the PR curve with professional styling\n",
        "ax2.plot(\n",
        "    recall,\n",
        "    precision,\n",
        "    color='#F18F01',  # Vibrant orange color\n",
        "    linewidth=2.5,\n",
        "    label=f'PR Curve (AP = {average_precision:.4f})',\n",
        "    alpha=0.9)\n",
        "\n",
        "# Customize PR plot appearance\n",
        "ax2.set_xlabel('Recall (Sensitivity)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Precision (Positive Predictive Value)',\n",
        "               fontsize=12,\n",
        "               fontweight='bold')\n",
        "ax2.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold', pad=20)\n",
        "ax2.legend(loc='best', fontsize=11, framealpha=0.9)\n",
        "ax2.grid(True, alpha=0.3, linestyle=':', linewidth=0.8)\n",
        "ax2.set_xlim([0.0, 1.0])\n",
        "ax2.set_ylim([0.0, 1.05])\n",
        "\n",
        "# Add subtle background color to emphasize good performance region\n",
        "ax2.fill_between(recall, precision, alpha=0.1, color='#F18F01')\n",
        "\n",
        "# Adjust layout to prevent label cutoff and ensure proper spacing\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"MODEL PERFORMANCE SUMMARY\")\n",
        "print(f\"  ROC-AUC Score:        {roc_auc:.4f}\")\n",
        "print(f\"  Average Precision:    {average_precision:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xodInvKmFaBM"
      },
      "source": [
        "###  Feature Importance Analysis\n",
        "\n",
        "* Higher values indicate features that contribute more to predictions.\n",
        "* Top features are the most influential in model decisions.\n",
        "* Features with zero importance can potentially be removed to simplify the model.\n",
        "* Use this analysis for feature selection and model interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7FasJ8HFaBM"
      },
      "outputs": [],
      "source": [
        "# Feature Importance Analysis with Grouped Encoded Features\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "\n",
        "# Extract raw feature importance scores\n",
        "# Using 'weight' (frequency) - number of times feature is used in splits\n",
        "feature_importance = xgb_model.get_booster().get_score(\n",
        "    importance_type='weight')\n",
        "\n",
        "\n",
        "# Map feature names to their base names (grouping encoded variants)\n",
        "def get_base_feature_name(feature_name):\n",
        "  \"\"\"\n",
        "    Group encoded features by their original categorical variable.\n",
        "\n",
        "    Examples:\n",
        "        'Department_Sales' -> 'Department'\n",
        "        'MaritalStatus_Single' -> 'MaritalStatus'\n",
        "        'BusinessTravel_label' -> 'BusinessTravel'\n",
        "        'Gender_bin' -> 'Gender'\n",
        "        'EducationField_freq' -> 'EducationField'\n",
        "    \"\"\"\n",
        "  # One-hot encoded features (contain underscore with category name)\n",
        "  if 'Department_' in feature_name:\n",
        "    return 'Department'\n",
        "  elif 'MaritalStatus_' in feature_name:\n",
        "    return 'MaritalStatus'\n",
        "  # Suffix-based encoded features\n",
        "  elif feature_name.endswith('_label'):\n",
        "    return feature_name.replace('_label', '')\n",
        "  elif feature_name.endswith('_freq'):\n",
        "    return feature_name.replace('_freq', '')\n",
        "  elif feature_name.endswith('_bin'):\n",
        "    return feature_name.replace('_bin', '')\n",
        "  # Original numerical features (no encoding)\n",
        "  else:\n",
        "    return feature_name\n",
        "\n",
        "\n",
        "# Group importance scores by base feature name\n",
        "grouped_importance = {}\n",
        "for feature, score in feature_importance.items():\n",
        "  base_name = get_base_feature_name(feature)\n",
        "  if base_name not in grouped_importance:\n",
        "    grouped_importance[base_name] = 0\n",
        "  grouped_importance[base_name] += score\n",
        "\n",
        "# Sort by importance in descending order\n",
        "importance_sorted = sorted(grouped_importance.items(),\n",
        "                           key=lambda x: x[1],\n",
        "                           reverse=True)\n",
        "\n",
        "# Extract top N features for visualization\n",
        "n_features_to_plot = min(20, len(importance_sorted))\n",
        "features = [item[0] for item in importance_sorted[:n_features_to_plot]]\n",
        "importance_values = [\n",
        "    item[1] for item in importance_sorted[:n_features_to_plot]\n",
        "]\n",
        "\n",
        "# Create professional bar chart\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Create color gradient based on importance\n",
        "colors = plt.cm.viridis(np.linspace(0.3, 0.9, n_features_to_plot))\n",
        "\n",
        "bars = ax.barh(range(n_features_to_plot),\n",
        "               importance_values,\n",
        "               color=colors,\n",
        "               alpha=0.85)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, value) in enumerate(zip(bars, importance_values)):\n",
        "  ax.text(value + max(importance_values) * 0.01,\n",
        "          i,\n",
        "          f'{value:.0f}',\n",
        "          va='center',\n",
        "          fontsize=9,\n",
        "          fontweight='bold')\n",
        "\n",
        "ax.set_yticks(range(n_features_to_plot))\n",
        "ax.set_yticklabels(features, fontsize=10)\n",
        "\n",
        "ax.set_xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Features', fontsize=12, fontweight='bold')\n",
        "ax.set_title(f'Top {n_features_to_plot} Most Important Features',\n",
        "             fontsize=14,\n",
        "             fontweight='bold',\n",
        "             pad=15)\n",
        "ax.invert_yaxis()  # Highest importance at top\n",
        "ax.grid(axis='x', alpha=0.3, linestyle=':', linewidth=0.8)\n",
        "ax.set_facecolor('#f8f9fa')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed feature importance table\n",
        "print(f\"\\nTop {n_features_to_plot} Feature Importance Scores:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'Rank':<6} {'Feature Name':<30} {'Importance Score':>15}\")\n",
        "print()\n",
        "for i, (feature, score) in enumerate(importance_sorted[:n_features_to_plot],\n",
        "                                     1):\n",
        "  print(f\"{i:<6} {feature:<30} {score:>15.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-ulFQrxFaBU"
      },
      "source": [
        "## Module 4: Causal Analysis\n",
        "\n",
        "**Objective** Evaluate the causal relationships between selected features (treatments) and outcome variables, isolating their independent effect from correlations captured by predictive models.\n",
        "\n",
        "**Output** Includes estimates of causal effects such as the Average Treatment Effect (ATE), Conditional Average Treatment Effect (CATE), or other relevant causal metrics. Visualizations may include causal graphs, treatment effect distributions, and sensitivity analyses to assess robustness. The module also provides a detailed interpretation of how individual features influence the outcome independently of predictive associations, highlighting actionable insights for decision-making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqBr4UABFaBV"
      },
      "source": [
        "##### Purpose\n",
        "- **Correlation**: Traditional ML finds patterns but can't prove causation\n",
        "- **Causal Inference**: Identifies what changes in X actually *cause* changes in Y\n",
        "\n",
        "##### DoWhy Framework Steps\n",
        "- **Model**: Specify causal graph with treatment, outcome, confounders\n",
        "- **Identify**: Find causal estimand (what to measure)\n",
        "- **Estimate**: Quantify the causal effect\n",
        "- **Refute**: Validate using sensitivity tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQV-viWeFaBV"
      },
      "source": [
        "### Causal Analysis of Employee Attrition Using DoWhy\n",
        "\n",
        "Implements an enhanced causal analysis pipeline for employee attrition using the DoWhy library. Its main purpose is to identify and quantify the causal effects of various factors on employee turnover, rather than just correlations.\n",
        "\n",
        "**1. Data Preparation**\n",
        "*  Converts categorical variables to numeric formats for causal modeling.\n",
        "*  Drops non-informative columns and handles missing values.\n",
        "*  Generates new binary treatment variables from continuous variables (e.g., HighIncome, LongTenure, FarFromHome) to improve causal effect estimation.*\n",
        "\n",
        "**2. Multiple Treatment Effects Analysis**\n",
        "* Defines multiple treatments (interventions) such as OverTime, JobSatisfaction, WorkLifeBalance, etc.\n",
        "* Selects relevant confounders for each treatment—variables that affect both the treatment and the outcome—to ensure valid causal inference.\n",
        "* Estimates Average Treatment Effects (ATE) using propensity score matching or linear regression.\n",
        "\n",
        "**3. Visualization of Causal Effects**\n",
        "* Creates a horizontal bar chart showing the estimated ATE for each intervention.\n",
        "* Uses color coding to represent the magnitude and direction of effects (strong positive, moderate positive, moderate negative, strong negative).\n",
        "* Adds numeric labels on bars for precise interpretation.\n",
        "\n",
        "**4. Effect Interpretation and Recommendations**\n",
        "* Sorts factors by their impact on attrition risk.\n",
        "* Provides actionable HR recommendations based on the strongest causal factors, such as reducing overtime, improving work-life balance, or offering competitive compensation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRXLF-64FaBV"
      },
      "outputs": [],
      "source": [
        "# Employee Attrition Causal Analysis using DoWhy\n",
        "\n",
        "\n",
        "from dowhy import CausalModel\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Data Preparation for Causal Analysis\n",
        "\n",
        "\n",
        "def prepare_causal_data(df):\n",
        "  \"\"\"\n",
        "  Prepare data for causal analysis with feature engineering.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create a copy to avoid modifying original\n",
        "  df_causal = df.copy()\n",
        "\n",
        "  # Convert binary categorical variables to numeric\n",
        "  binary_mappings = {\n",
        "      'Attrition': {\n",
        "          'Yes': 1,\n",
        "          'No': 0\n",
        "      },\n",
        "      'OverTime': {\n",
        "          'Yes': 1,\n",
        "          'No': 0\n",
        "      },\n",
        "      'Gender': {\n",
        "          'Male': 1,\n",
        "          'Female': 0\n",
        "      }\n",
        "  }\n",
        "\n",
        "  for col, mapping in binary_mappings.items():\n",
        "    if col in df_causal.columns:\n",
        "      df_causal[col] = df_causal[col].map(mapping)\n",
        "\n",
        "  # For categorical variables with more categories, use numeric encoding\n",
        "  if 'BusinessTravel' in df_causal.columns:\n",
        "    travel_map = {'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2}\n",
        "    df_causal['BusinessTravel'] = df_causal['BusinessTravel'].map(travel_map)\n",
        "\n",
        "  # Drop non-informative columns\n",
        "  cols_to_drop = ['EmployeeNumber', 'Over18', 'StandardHours', 'EmployeeCount']\n",
        "  df_causal = df_causal.drop(\n",
        "      columns=[c for c in cols_to_drop if c in df_causal.columns])\n",
        "\n",
        "  # Keep only numeric columns for simplicity\n",
        "  numeric_cols = df_causal.select_dtypes(include=[np.number]).columns\n",
        "  df_causal = df_causal[numeric_cols]\n",
        "\n",
        "  # Create binned versions of continuous variables for treatment analysis\n",
        "  # This improves causal estimation for continuous variables\n",
        "  if 'MonthlyIncome' in df_causal.columns:\n",
        "    df_causal['HighIncome'] = (\n",
        "        df_causal['MonthlyIncome']\n",
        "        > df_causal['MonthlyIncome'].median()).astype(int)\n",
        "\n",
        "  if 'YearsAtCompany' in df_causal.columns:\n",
        "    df_causal['LongTenure'] = (df_causal['YearsAtCompany'] > 5).astype(int)\n",
        "\n",
        "  if 'DistanceFromHome' in df_causal.columns:\n",
        "    df_causal['FarFromHome'] = (\n",
        "        df_causal['DistanceFromHome']\n",
        "        > df_causal['DistanceFromHome'].median()).astype(int)\n",
        "\n",
        "  # Remove any rows with missing values\n",
        "  df_causal = df_causal.dropna()\n",
        "\n",
        "  print(\"DATA PREPARATION FOR CAUSAL ANALYSIS\")\n",
        "  print(f\"Prepared causal dataset: {df_causal.shape}\")\n",
        "  print(f\"Attrition rate: {df_causal['Attrition'].mean():.2%}\")\n",
        "  print(f\"Binary treatment variables created:\")\n",
        "  if 'HighIncome' in df_causal.columns:\n",
        "    print(f\"  HighIncome: {df_causal['HighIncome'].mean():.1%} of employees\")\n",
        "  if 'LongTenure' in df_causal.columns:\n",
        "    print(f\"  LongTenure: {df_causal['LongTenure'].mean():.1%} of employees\")\n",
        "  if 'FarFromHome' in df_causal.columns:\n",
        "    print(\n",
        "        f\"  FarFromHome: {df_causal['FarFromHome'].mean():.1%} of employees\")\n",
        "  print(\"-\" * 70 + \"\\n\")\n",
        "\n",
        "  return df_causal\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Enhanced Multiple Treatment Effects Analysis\n",
        "\n",
        "\n",
        "def analyze_multiple_treatments(df_causal):\n",
        "  \"\"\"\n",
        "  Analyze causal effects of multiple interventions with improved confounder selection.\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"COMPREHENSIVE CAUSAL EFFECTS ANALYSIS\")\n",
        "\n",
        "  # Define treatments with carefully selected confounders\n",
        "  # Key principle: confounders must affect BOTH treatment AND outcome\n",
        "  treatments = {\n",
        "      'OverTime': {\n",
        "          'confounders': [\n",
        "              'Age', 'JobLevel', 'MonthlyIncome', 'YearsAtCompany',\n",
        "              'JobSatisfaction', 'WorkLifeBalance'\n",
        "          ],\n",
        "          'method':\n",
        "          'backdoor.propensity_score_matching'\n",
        "      },\n",
        "      'JobSatisfaction': {\n",
        "          'confounders': [\n",
        "              'Age', 'MonthlyIncome', 'YearsAtCompany', 'JobLevel',\n",
        "              'EnvironmentSatisfaction', 'WorkLifeBalance'\n",
        "          ],\n",
        "          'method':\n",
        "          'backdoor.propensity_score_matching'\n",
        "      },\n",
        "      'WorkLifeBalance': {\n",
        "          'confounders': [\n",
        "              'Age', 'MonthlyIncome', 'YearsAtCompany', 'JobLevel',\n",
        "              'JobSatisfaction', 'EnvironmentSatisfaction'\n",
        "          ],\n",
        "          'method':\n",
        "          'backdoor.propensity_score_matching'\n",
        "      },\n",
        "      'EnvironmentSatisfaction': {\n",
        "          'confounders': [\n",
        "              'Age', 'MonthlyIncome', 'YearsAtCompany', 'JobLevel',\n",
        "              'JobSatisfaction', 'WorkLifeBalance'\n",
        "          ],\n",
        "          'method':\n",
        "          'backdoor.propensity_score_matching'\n",
        "      },\n",
        "      'HighIncome': {\n",
        "          'confounders': [\n",
        "              'Age', 'Education', 'JobLevel', 'YearsAtCompany',\n",
        "              'PerformanceRating', 'TotalWorkingYears'\n",
        "          ],\n",
        "          'method':\n",
        "          'backdoor.propensity_score_matching'\n",
        "      },\n",
        "      'LongTenure': {\n",
        "          'confounders': [\n",
        "              'Age', 'JobLevel', 'MonthlyIncome', 'JobSatisfaction',\n",
        "              'EnvironmentSatisfaction', 'WorkLifeBalance'\n",
        "          ],\n",
        "          'method':\n",
        "          'backdoor.propensity_score_matching'\n",
        "      },\n",
        "      'FarFromHome': {\n",
        "          'confounders': [\n",
        "              'Age', 'MonthlyIncome', 'JobLevel', 'MaritalStatus',\n",
        "              'WorkLifeBalance', 'JobSatisfaction'\n",
        "          ],\n",
        "          'method':\n",
        "          'backdoor.propensity_score_matching'\n",
        "      },\n",
        "      'BusinessTravel': {\n",
        "          'confounders': [\n",
        "              'Age', 'JobLevel', 'MonthlyIncome', 'YearsAtCompany',\n",
        "              'JobSatisfaction', 'WorkLifeBalance'\n",
        "          ],\n",
        "          'method':\n",
        "          'backdoor.linear_regression'\n",
        "      }\n",
        "  }\n",
        "\n",
        "  results = {}\n",
        "\n",
        "  for treatment, config in treatments.items():\n",
        "\n",
        "    confounders = config['confounders']\n",
        "    method = config['method']\n",
        "\n",
        "    # Filter confounders that exist in dataset\n",
        "    available_confounders = [c for c in confounders if c in df_causal.columns]\n",
        "\n",
        "    if len(available_confounders) < 2:\n",
        "      print(f\" Skipping {treatment}: Insufficient confounders\")\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      # Create causal model\n",
        "      model = CausalModel(data=df_causal,\n",
        "                          treatment=treatment,\n",
        "                          outcome='Attrition',\n",
        "                          common_causes=available_confounders)\n",
        "\n",
        "      # Identify causal estimand\n",
        "      identified_estimand = model.identify_effect(\n",
        "          proceed_when_unidentifiable=True)\n",
        "\n",
        "      # Estimate causal effect\n",
        "      estimate = model.estimate_effect(identified_estimand, method_name=method)\n",
        "\n",
        "      # Store results\n",
        "      results[treatment] = {\n",
        "          'ate': estimate.value,\n",
        "          'method': method,\n",
        "          'n_confounders': len(available_confounders)\n",
        "      }\n",
        "\n",
        "      print(\n",
        "          f\"✓ {treatment:25s}: ATE = {estimate.value:>8.4f}  ({len(available_confounders)} confounders)\"\n",
        "      )\n",
        "\n",
        "    except Exception as e:\n",
        "      error_msg = str(e)[:60]\n",
        "      print(f\"✗ {treatment:25s}: Failed - {error_msg}\")\n",
        "      continue\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Visualization with Effect Sizes\n",
        "\n",
        "\n",
        "def visualize_causal_effects_enhanced(effects_dict, top_n=10):\n",
        "  \"\"\"\n",
        "  Create visualization showing causal effects with better design.\n",
        "  \"\"\"\n",
        "\n",
        "  # Extract ATE values\n",
        "  ate_data = [(k, v['ate']) for k, v in effects_dict.items()]\n",
        "\n",
        "  # Sort by absolute effect size\n",
        "  sorted_effects = sorted(ate_data, key=lambda x: abs(x[1]),\n",
        "                          reverse=True)[:top_n]\n",
        "  treatments = [item[0] for item in sorted_effects]\n",
        "  effects = [item[1] for item in sorted_effects]\n",
        "\n",
        "  # Create figure with better layout\n",
        "  fig, ax = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "  # Color based on direction and magnitude\n",
        "  colors = []\n",
        "  for e in effects:\n",
        "    if e > 0.1:\n",
        "      colors.append('#C1121F')  # Strong red for large positive effects\n",
        "    elif e > 0:\n",
        "      colors.append('#E63946')  # Light red for small positive effects\n",
        "    elif e > -0.1:\n",
        "      colors.append('#2A9D8F')  # Light green for small negative effects\n",
        "    else:\n",
        "      colors.append('#06604E')  # Dark green for large negative effects\n",
        "\n",
        "  # Create horizontal bar chart\n",
        "  bars = ax.barh(treatments,\n",
        "                 effects,\n",
        "                 color=colors,\n",
        "                 alpha=0.85,\n",
        "                 edgecolor='black',\n",
        "                 linewidth=1.5)\n",
        "\n",
        "  # Add value labels with better formatting\n",
        "  for bar, value in zip(bars, effects):\n",
        "    width = bar.get_width()\n",
        "    label_x = width + (0.002 if width > 0 else -0.002)\n",
        "    ha = 'left' if width > 0 else 'right'\n",
        "\n",
        "    # Format label with sign\n",
        "    label_text = f'+{value:.3f}' if value > 0 else f'{value:.3f}'\n",
        "\n",
        "    ax.text(label_x,\n",
        "            bar.get_y() + bar.get_height() / 2,\n",
        "            label_text,\n",
        "            ha=ha,\n",
        "            va='center',\n",
        "            fontweight='bold',\n",
        "            fontsize=11,\n",
        "            bbox=dict(boxstyle='round,pad=0.3',\n",
        "                      facecolor='white',\n",
        "                      alpha=0.8,\n",
        "                      edgecolor='none'))\n",
        "\n",
        "  # Add reference line at 0\n",
        "  ax.axvline(x=0,\n",
        "             color='black',\n",
        "             linestyle='-',\n",
        "             linewidth=2.5,\n",
        "             alpha=0.8,\n",
        "             zorder=0)\n",
        "\n",
        "  # Customize axes\n",
        "  ax.set_xlabel('Average Treatment Effect (ATE) on Attrition Probability',\n",
        "                fontsize=13,\n",
        "                fontweight='bold',\n",
        "                labelpad=12)\n",
        "  ax.set_ylabel('Intervention / Treatment Variable',\n",
        "                fontsize=13,\n",
        "                fontweight='bold',\n",
        "                labelpad=12)\n",
        "  ax.set_title(\n",
        "      'Causal Effects on Employee Attrition (Ranked by Impact)\\n'\n",
        "      'Positive Values = Increases Attrition Risk | Negative Values = Reduces Attrition Risk',\n",
        "      fontsize=14,\n",
        "      fontweight='bold',\n",
        "      pad=20)\n",
        "\n",
        "  # Improve grid\n",
        "  ax.grid(axis='x', alpha=0.3, linestyle='--', linewidth=1)\n",
        "  ax.set_axisbelow(True)\n",
        "\n",
        "  # Add legend with better styling\n",
        "  legend_elements = [\n",
        "      Patch(facecolor='#C1121F',\n",
        "            label='Strong Positive Effect (> +0.10)',\n",
        "            alpha=0.85),\n",
        "      Patch(facecolor='#E63946',\n",
        "            label='Moderate Positive Effect (0 to +0.10)',\n",
        "            alpha=0.85),\n",
        "      Patch(facecolor='#2A9D8F',\n",
        "            label='Moderate Negative Effect (0 to -0.10)',\n",
        "            alpha=0.85),\n",
        "      Patch(facecolor='#06604E',\n",
        "            label='Strong Negative Effect (< -0.10)',\n",
        "            alpha=0.85)\n",
        "  ]\n",
        "  ax.legend(handles=legend_elements,\n",
        "            loc='best',\n",
        "            fontsize=10,\n",
        "            framealpha=0.95,\n",
        "            edgecolor='black')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Effect Size Interpretation and Recommendations\n",
        "\n",
        "\n",
        "def interpret_causal_effects(effects_dict):\n",
        "  \"\"\"\n",
        "  Provide interpretation and actionable recommendations.\n",
        "  \"\"\"\n",
        "\n",
        "  # Extract ATE values\n",
        "  ate_data = [(k, v['ate']) for k, v in effects_dict.items()]\n",
        "  sorted_effects = sorted(ate_data, key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "  print(\"CAUSAL EFFECTS INTERPRETATION & RECOMMENDATIONS\")\n",
        "\n",
        "  print(\"\\n Top Factors Increasing Attrition Risk:\")\n",
        "  positive_effects = [(t, e) for t, e in sorted_effects if e > 0]\n",
        "  if positive_effects:\n",
        "    for i, (treatment, effect) in enumerate(positive_effects[:5], 1):\n",
        "      impact = \"STRONG\" if abs(effect) > 0.1 else \"MODERATE\"\n",
        "      print(\n",
        "          f\"{i}. {treatment:25s}: +{effect:.4f} ({impact} impact).      Increases attrition probability by {effect*100:.2f} percentage points\"\n",
        "      )\n",
        "  else:\n",
        "    print(\"No factors found that increase attrition\")\n",
        "\n",
        "  print(\"-\" * 70)\n",
        "  print(\"\\n Top Factors Reducing Attrition Risk:\")\n",
        "\n",
        "  negative_effects = [(t, e) for t, e in sorted_effects if e < 0]\n",
        "  if negative_effects:\n",
        "    for i, (treatment, effect) in enumerate(negative_effects[:5], 1):\n",
        "      impact = \"STRONG\" if abs(effect) > 0.1 else \"MODERATE\"\n",
        "      print(\n",
        "          f\"{i}. {treatment:25s}: {effect:.4f} ({impact} impact).       Reduces attrition probability by {abs(effect)*100:.2f} percentage points\"\n",
        "      )\n",
        "  else:\n",
        "    print(\"No factors found that reduce attrition\")\n",
        "\n",
        "  print(\"-\" * 70)\n",
        "  print(\"\\nActionable HR Interventions (Ranked by Impact):\")\n",
        "\n",
        "\n",
        "  # Generate recommendations based on top effects\n",
        "  recommendations = []\n",
        "\n",
        "  for treatment, effect in sorted_effects[:3]:\n",
        "    if 'OverTime' in treatment and effect > 0:\n",
        "      recommendations.append(\n",
        "          f\"• REDUCE OVERTIME: Has strong causal effect (+{effect:.3f}). \"\n",
        "          f\"Implement overtime monitoring and workload rebalancing.\")\n",
        "    elif 'Satisfaction' in treatment:\n",
        "      if effect > 0:\n",
        "        recommendations.append(\n",
        "            f\"• IMPROVE {treatment.upper()}: Low satisfaction causally increases \"\n",
        "            f\"attrition (+{effect:.3f}). Conduct engagement surveys and act on feedback.\"\n",
        "        )\n",
        "      else:\n",
        "        recommendations.append(\n",
        "            f\"• MAINTAIN {treatment.upper()}: Protective factor ({effect:.3f}). \"\n",
        "            f\"Continue current satisfaction initiatives.\")\n",
        "    elif 'Income' in treatment or 'HighIncome' in treatment:\n",
        "      if effect < 0:\n",
        "        recommendations.append(\n",
        "            f\"• COMPETITIVE COMPENSATION: Higher income reduces attrition ({effect:.3f}). \"\n",
        "            f\"Review salary benchmarks and retention bonuses.\")\n",
        "    elif 'Tenure' in treatment or 'LongTenure' in treatment:\n",
        "      if effect < 0:\n",
        "        recommendations.append(\n",
        "            f\"• RETENTION PROGRAMS: Tenure reduces attrition ({effect:.3f}). \"\n",
        "            f\"Implement long-term incentives and career development.\")\n",
        "    elif 'Balance' in treatment and effect > 0:\n",
        "      recommendations.append(\n",
        "          f\"• WORK-LIFE BALANCE: Critical factor (+{effect:.3f}). \"\n",
        "          f\"Introduce flexible work arrangements and wellness programs.\")\n",
        "\n",
        "  for rec in recommendations:\n",
        "    print(rec)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Main Execution Function\n",
        "\n",
        "\n",
        "def run_enhanced_causal_analysis(df):\n",
        "  \"\"\"\n",
        "  Execute complete enhanced causal analysis pipeline.\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"ENHANCED EMPLOYEE ATTRITION CAUSAL ANALYSIS\")\n",
        "  print()\n",
        "\n",
        "  # Step 1: Prepare data\n",
        "  df_causal = prepare_causal_data(df)\n",
        "\n",
        "  # Step 2: Analyze multiple treatments\n",
        "  effects = analyze_multiple_treatments(df_causal)\n",
        "\n",
        "  if not effects:\n",
        "    print(\"No causal effects could be estimated. Check data quality.\")\n",
        "    return df_causal, effects\n",
        "\n",
        "  # Step 3: Visualize results\n",
        "  print(\"\\n Creating causal effects visualization...\\n\")\n",
        "  visualize_causal_effects_enhanced(effects, top_n=10)\n",
        "\n",
        "  # Step 4: Interpret and provide recommendations\n",
        "  interpret_causal_effects(effects)\n",
        "\n",
        "  return df_causal, effects\n",
        "\n",
        "\n",
        "df_causal, causal_effects = run_enhanced_causal_analysis(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tj_lKCWFaBV"
      },
      "source": [
        "##### Statistical Notes\n",
        "- **ATE** = Average Treatment Effect (percentage point change)\n",
        "- **Positive ATE** = increases attrition probability\n",
        "- **Negative ATE** = decreases attrition probability"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}